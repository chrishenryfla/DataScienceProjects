{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e083b6",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc6a3c2",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from static_grader import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc5629",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Image Classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce44793e",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Image classification is a common task for deep learning and neural networks.  The raw features coming in are the pixel values.  These are simple enough to deal with, but it is difficult to connect pixel values to determining whether an image is of a cat.  Older methods used a lot of clever filters, but the current best-of-breed algorithms simply throw a lot of linear algebra at the problem.\n",
    "\n",
    "In this miniproject, you build a series of models to classify a series of images into one of ten classes. For expediency, these images are pretty small ($32\\times32\\times3$).  This can make classification a bit tricky&mdash;human performance is only about 94%.  Each of your models will be scored by comparing its accuracy to the accuracy of a reference model that we developed.  A score of 1 indicates that your model performs as well as the reference model; not that your accuracy is 100%!\n",
    "\n",
    "You will be given both a training set and a test set.  Ground truth values are provided for the training set.  You should train your models on this set, and then make predictions for each of the test images.  These predictions will be submitted to the grader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae8e93",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## A note on scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad6c6e",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "It **is** possible to score above 1 on these questions. This indicates that you've beaten our reference model&mdash;we compare our model's score on a test set to your score on a test set. See how high you can go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e40ea",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8a4ce",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We will be using the `CIFAR-10` data set.  It consists of 60,000 images, each $32\\times32$ color pixels, each belonging to one of ten classes.  The following cell will download the data, in NumPy's `.npy` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bec4df",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "!aws s3 sync s3://dataincubator-course/cifar10/ . --exclude 'valid*'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff135df0",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We can load in the data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4783d8",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "train_images = np.load(gzip.open('train_images.npy.gz', 'rb'))\n",
    "train_labels = np.load(gzip.open('train_labels.npy.gz', 'rb'))\n",
    "test_images = np.load(gzip.open('test_images.npy.gz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9312a80f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7fe75",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The images are stored as four-dimensional arrays.  The first index indicates the image number, the second and third the $x$ and $y$ positions, and the fourth index the color channel.  Each pixel color is a floating point number between 0 and 1.  This convention allows us to view the images with matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb699d00",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6cb0f0cee0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "matplotlib.pyplot.rcParams[\"axes.grid\"] = False  #  Remove the grid lines from the image.\n",
    "matplotlib.pyplot.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f860ad2",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The classes have already been numbered 0-9 for us; those numbers are stored in the vector `train_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63cc0fc2",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05928fa2",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The human-readable names associated with this classes are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00879e1b",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c722826e",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names[train_labels[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d4360",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "So we can see that the image above is a frog.  (Now you see it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "673e9dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.04313726, 0.05490196, 0.09411765],\n",
       "        [0.04705882, 0.05882353, 0.09411765],\n",
       "        [0.03921569, 0.05098039, 0.08627451],\n",
       "        ...,\n",
       "        [0.03529412, 0.03921569, 0.05882353],\n",
       "        [0.03137255, 0.03529412, 0.05490196],\n",
       "        [0.03137255, 0.03529412, 0.05490196]],\n",
       "\n",
       "       [[0.04313726, 0.05490196, 0.09019608],\n",
       "        [0.03921569, 0.05098039, 0.08627451],\n",
       "        [0.04313726, 0.05490196, 0.09019608],\n",
       "        ...,\n",
       "        [0.03137255, 0.03529412, 0.05490196],\n",
       "        [0.02745098, 0.03137255, 0.05098039],\n",
       "        [0.03137255, 0.03529412, 0.05490196]],\n",
       "\n",
       "       [[0.04705882, 0.05882353, 0.09411765],\n",
       "        [0.04313726, 0.05490196, 0.09019608],\n",
       "        [0.04313726, 0.05490196, 0.09019608],\n",
       "        ...,\n",
       "        [0.03137255, 0.03529412, 0.05490196],\n",
       "        [0.03137255, 0.03529412, 0.05490196],\n",
       "        [0.03137255, 0.03529412, 0.05490196]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.972549  , 0.9529412 , 0.91764706],\n",
       "        [0.92941177, 0.8980392 , 0.85882354],\n",
       "        [0.8745098 , 0.83137256, 0.8156863 ],\n",
       "        ...,\n",
       "        [0.07058824, 0.07450981, 0.09019608],\n",
       "        [0.03529412, 0.04313726, 0.0627451 ],\n",
       "        [0.04313726, 0.04705882, 0.06666667]],\n",
       "\n",
       "       [[0.98039216, 0.9647059 , 0.93333334],\n",
       "        [0.91764706, 0.89411765, 0.85882354],\n",
       "        [0.85490197, 0.8156863 , 0.8039216 ],\n",
       "        ...,\n",
       "        [0.03137255, 0.03137255, 0.05098039],\n",
       "        [0.03921569, 0.04313726, 0.0627451 ],\n",
       "        [0.04313726, 0.04705882, 0.06666667]],\n",
       "\n",
       "       [[0.98039216, 0.972549  , 0.94509804],\n",
       "        [0.92156863, 0.90588236, 0.8745098 ],\n",
       "        [0.8627451 , 0.83137256, 0.8235294 ],\n",
       "        ...,\n",
       "        [0.03921569, 0.04313726, 0.0627451 ],\n",
       "        [0.03529412, 0.04313726, 0.0627451 ],\n",
       "        [0.03921569, 0.04313726, 0.0627451 ]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062bfb2f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2d9e9",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Perceptual Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf9c816",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "Since we already have a number of labeled images, a simple approach would be to measure the difference between two images, and choose the label corresponding to nearby images.  To do this, we need to develop a metric to determine the distance between two images.  We'll make the simplifying (and completely wrong) assumption that this is just the average difference between the colors of the corresponding pixels in the two images.\n",
    "\n",
    "While we could just take RMS difference of the red, green, and blue pixels, let's be slightly more sophisticated and look at human vision for a metric.  After all, we're pretty good at image classifications, so there might be some useful optimization here.\n",
    "\n",
    "It turns out that modeling human perception is [extraordinarily complicated](https://en.wikipedia.org/wiki/Color_difference#CIEDE2000).  We're going to use a [simplified model](https://www.compuphase.com/cmetric.htm):\n",
    "\n",
    "$$\\Delta C \\equiv \\sqrt{2 \\Delta R^2 + 4 \\Delta G^2 + 3 \\Delta B^2 + \\bar R\\left(\\Delta R^2 - \\Delta B^2 \\right)} $$\n",
    "where $(R_1, G_1, B_1)$ and $(R_2, G_2, B_2)$ are the RGB components of the two colors and\n",
    "$$\\begin{align}\n",
    "\\Delta R &= R_1 - R_2 \\\\\n",
    "\\Delta G &= G_1 - G_2 \\\\\n",
    "\\Delta B &= B_1 - B_2 \\\\\n",
    "\\bar R &= \\textstyle\\frac{1}{2}\\left(R_1 + R_2\\right)\n",
    "\\end{align}$$\n",
    "\n",
    "This accounts for the fact that our eyes are most sensitive to green and least sensitive to red, and that perception is not constant with hue.\n",
    "\n",
    "Build a graph that takes in a series of images, as well as a base image, and returns a result containing the $\\Delta C$ value (for each pixel) between the base image and each image in the series.  \n",
    "\n",
    "(Note that the intention here in our solution is that `images` will be a stack of images, while `base` is a single image.  Since the function is decorated with `tf.function` the computations within the function will be recorded in a computation graph and the output from this function will be a tensor.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b148a0ed",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def delta_func(base, images):\n",
    "    # Convert images to float32\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    base = tf.cast(base, tf.float32)\n",
    "\n",
    "    # Calculate differences in red, green, and blue channels\n",
    "    delta_r = images[:, :, :, 0] - base[:, :, :, 0]\n",
    "    delta_g = images[:, :, :, 1] - base[:, :, :, 1]\n",
    "    delta_b = images[:, :, :, 2] - base[:, :, :, 2]\n",
    "\n",
    "    # Calculate average red channel\n",
    "    avg_r = (images[:, :, :, 0] + base[:, :, :, 0]) / 2.0\n",
    "\n",
    "    # Calculate Δ𝐶 using the provided formula\n",
    "    delta_c = tf.sqrt(2 * tf.square(delta_r) + 4 * tf.square(delta_g) + 3 * tf.square(delta_b) + avg_r * (tf.square(delta_r) - tf.square(delta_b)))\n",
    "\n",
    "    return delta_c\n",
    "\n",
    "@tf.function\n",
    "def compute_distance(test_images, training_images):\n",
    "    num_training_images = tf.shape(training_images)[0]  # Get the number of training images\n",
    "    \n",
    "    # Reshape and tile the base test image to match the number of training images\n",
    "    base_test_image = test_images[0:1, :, :, :]\n",
    "    tiled_base_test_image = tf.tile(base_test_image, [num_training_images, 1, 1, 1])\n",
    "    \n",
    "    delta = delta_func(tiled_base_test_image, training_images)  # Compute delta for all training images\n",
    "    \n",
    "    # Compute the average delta value for each training image\n",
    "    avg_deltas = tf.reduce_mean(delta, axis=[1, 2])  # Reduce along dimensions 1 and 2\n",
    "    \n",
    "    return avg_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac527e6",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def delta_func(images, base):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa64b1c",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "As stated, for two images, $I_1$ and $I_2$, we define the distance between $I_1$ and $I_2$ as the average $\\Delta C$ value over the whole image, that is:\n",
    "\n",
    "$$d(I_1, I_2) = \\frac{1}{N}\\sum_{p_j} \\Delta C(p_j)$$\n",
    "where the sum is over all pixels $p_j$, $\\Delta C(p_2)$ is the $\\Delta C$ value for the pixel $p_j$, and $N$ is the total number of pixels in each image ($N= 32\\times 32$ in our case).\n",
    "\n",
    "Using `delta_func` compute the distance between the first test image and all of the training images.\n",
    "\n",
    "**Checkpoint:** The mean value of the distances is 1.159, and the standard deviation of the distances is 0.182."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67334262",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "distances = compute_distance(test_images, train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648a757",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "deltas = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e829776",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "From these, find the 100 closest images from the training set to this first test image.  (Note that `numpy.argsort` might help here.)  Submit a list of the indices of these images to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47cb26ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Find the indices that would sort the distances in ascending order\n",
    "sorted_indices = np.argsort(distances)\n",
    "\n",
    "# Get the indices of the 100 closest images\n",
    "top_100_indices = sorted_indices[:100]\n",
    "\n",
    "# Now you can use these indices to retrieve the actual training images\n",
    "closest_images = train_images[top_100_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "855c898c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27912, 23437, 47732, 25053, 29251,  3616, 43445,  5555, 46911,\n",
       "       16269, 45450, 47893, 27833, 41078,  6385, 32788,  2108, 44366,\n",
       "       40857, 18619, 32596, 43840, 42187,  7982, 47629, 14298, 29521,\n",
       "       15860, 48219, 25953, 24522,  1363, 32891, 18702, 19759, 16247,\n",
       "        5859,  3303, 37982,  5463,  9738,  8448,  4881, 14185, 18347,\n",
       "       35163, 13960, 39700,  7837, 36899, 31793, 11317, 29615, 10413,\n",
       "       45178, 16021, 40089,  6224, 42620, 14932, 27543, 18868, 46440,\n",
       "       17882, 44700, 11784, 24729, 37317, 45209, 10709, 17501, 10440,\n",
       "       46378, 32430, 15659, 21665, 25680, 30766, 46671, 43551, 32145,\n",
       "       20364, 11465, 40643, 12229, 23235,  6878, 36196, 33346,  2986,\n",
       "       22061, 16130, 43791,  8380, 17921, 45047,  1582, 32047,  3060,\n",
       "        1015])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abb0ba47",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "indices = [43234]*100\n",
    "\n",
    "grader.score('tf__perceptual_delta', top_100_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23fa1e7",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "**Extension:** What does this suggest about the proper class for this image?\n",
    "\n",
    "> **Aside:** Essentially, we've started to implement a $k$-nearest neighbors algorithm, using this perceptual distance as our metric.  If we ran the difference between all of the test images and each of the training images, we could make a prediction from the nearest images for each.  Give it a try, if you're interested, but this miniproject is going to go in another direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b165c81",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Softmax model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbf6ca",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "We could use this delta function to measure distances from images with known classes, but that's not really the Deep Learning Way (TM).  Instead, we'll let neural networks figure out how to connect pixel values to classes.\n",
    "\n",
    "We'll start with something that barely qualifies as a neural network: a softmax classifier.  This has no hidden layers; instead a single dense layer takes $32\\times32\\times3$ inputs and produces 10 outputs, one for each class.  It should use a softmax activation and a `CategoricalCrossentropy` loss function.\n",
    "\n",
    "Build such a model and train it on the training data.  Then use this model to make a prediction on each of the 10,000 test images.\n",
    "\n",
    "**Hints:**\n",
    "- The labels are given as integers, but softmax expects one-hot encoding of the labels.  The `tf.one_hot` function can do the conversion.\n",
    "- TensorFlow's `.predict` method will return probabilities for each image being in each class, but the grader wants only the class prediction.  You may find the `np.argmax` function helpful in determining the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b83ca4",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1125/1125 [==============================] - 3s 2ms/step - loss: 1.9901 - accuracy: 0.3089 - val_loss: 1.9945 - val_accuracy: 0.3075\n",
      "Epoch 2/5\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 1.8960 - accuracy: 0.3478 - val_loss: 1.8862 - val_accuracy: 0.3620\n",
      "Epoch 3/5\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 1.8642 - accuracy: 0.3582 - val_loss: 1.9145 - val_accuracy: 0.3313\n",
      "Epoch 4/5\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 1.8456 - accuracy: 0.3650 - val_loss: 1.8086 - val_accuracy: 0.3913\n",
      "Epoch 5/5\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 1.8335 - accuracy: 0.3731 - val_loss: 1.8462 - val_accuracy: 0.3577\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 1.8050 - accuracy: 0.3768\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your training data and labels (replace these with your actual data)\n",
    "x_train = train_images\n",
    "y_train = train_labels\n",
    "\n",
    "\n",
    "# Preprocess labels with one-hot encoding\n",
    "num_classes = 10\n",
    "y_train_one_hot = tf.one_hot(y_train, num_classes).numpy()\n",
    "\n",
    "# Split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess labels with one-hot encoding\n",
    "y_train_one_hot = tf.one_hot(y_train, num_classes).numpy()\n",
    "y_test_one_hot = tf.one_hot(y_test, num_classes).numpy()\n",
    "\n",
    "# Build the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(32, 32, 3)),  # Flatten the input images\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Output layer with softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # Use softmax cross-entropy loss\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ab103f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load your test data (replace this with your actual test data)\n",
    "test_data = test_images\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# The 'predictions' array will contain the predicted class probabilities for each image\n",
    "# You can get the predicted classes by finding the index of the maximum probability\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d0b4d0",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.1164\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#predicted_classes = [0]*len(test_images)\n",
    "\n",
    "grader.score('tf__softmax', predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c894c",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Fully-connected model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1105386",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "Now, add a hidden layer to this network.  Train this network on the pixel values, and once again use it to predict the most likely class for each of the test images.\n",
    "\n",
    "**Hints:**\n",
    "- We found that adding more layers didn't help too much.\n",
    "- Watch out for overfitting.  Dropout can help with this.\n",
    "- The reference solution achieves an accuracy of about 44% on a training set and 41% on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08381e63",
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1125/1125 [==============================] - 5s 4ms/step - loss: 1.9381 - accuracy: 0.3018 - val_loss: 1.8739 - val_accuracy: 0.3195\n",
      "Epoch 2/5\n",
      "1125/1125 [==============================] - 4s 4ms/step - loss: 1.7969 - accuracy: 0.3594 - val_loss: 1.8145 - val_accuracy: 0.3528\n",
      "Epoch 3/5\n",
      "1125/1125 [==============================] - 4s 4ms/step - loss: 1.7360 - accuracy: 0.3818 - val_loss: 1.7179 - val_accuracy: 0.3775\n",
      "Epoch 4/5\n",
      "1125/1125 [==============================] - 4s 4ms/step - loss: 1.6956 - accuracy: 0.3935 - val_loss: 1.7033 - val_accuracy: 0.3840\n",
      "Epoch 5/5\n",
      "1125/1125 [==============================] - 4s 4ms/step - loss: 1.6703 - accuracy: 0.4043 - val_loss: 1.6614 - val_accuracy: 0.4050\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.6646 - accuracy: 0.3967\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your training data and labels (replace these with your actual data)\n",
    "x_train = train_images\n",
    "y_train = train_labels\n",
    "\n",
    "\n",
    "# Preprocess labels with one-hot encoding\n",
    "num_classes = 10\n",
    "y_train_one_hot = tf.one_hot(y_train, num_classes).numpy()\n",
    "\n",
    "# Split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess labels with one-hot encoding\n",
    "y_train_one_hot = tf.one_hot(y_train, num_classes).numpy()\n",
    "y_test_one_hot = tf.one_hot(y_test, num_classes).numpy()\n",
    "\n",
    "# Build the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(32, 32, 3)),  # Flatten the input images\n",
    "    tf.keras.layers.Dense(128, activation='relu'),    # Hidden layer with ReLU activation\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Output layer with softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # Use softmax cross-entropy loss\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02a4ac7b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load your test data (replace this with your actual test data)\n",
    "test_data = test_images\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# The 'predictions' array will contain the predicted class probabilities for each image\n",
    "# You can get the predicted classes by finding the index of the maximum probability\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b93492",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0958\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#predicted_classes = [0]*len(test_images)\n",
    "\n",
    "grader.score('tf__fully_connected', predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101a19e",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Convolutional model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319e9c5",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "Convolutional neural networks have had a lot of success in image classification.  Build a neural network with convolutional layers to improve the performance.\n",
    "\n",
    "**Hints:**\n",
    "- The reference solution uses two convolutional layers and two fully-connected layers.\n",
    "- We found success with the `AdamOptimizer`.\n",
    "- The reference solution achieves an accuracy of roughly 80% on a training set and 70% on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c755f37",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.4387 - accuracy: 0.4809 - val_loss: 1.2186 - val_accuracy: 0.5709\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.0583 - accuracy: 0.6294 - val_loss: 1.0268 - val_accuracy: 0.6400\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.8976 - accuracy: 0.6844 - val_loss: 0.9870 - val_accuracy: 0.6558\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.7887 - accuracy: 0.7243 - val_loss: 0.9074 - val_accuracy: 0.6852\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.6997 - accuracy: 0.7576 - val_loss: 0.8651 - val_accuracy: 0.6995\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.8651 - accuracy: 0.6995\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your training data and labels (replace these with your actual data)\n",
    "x_train = train_images\n",
    "y_train = train_labels\n",
    "\n",
    "\n",
    "# Preprocess labels with one-hot encoding\n",
    "num_classes = 10\n",
    "y_train_one_hot = tf.one_hot(y_train, num_classes).numpy()\n",
    "\n",
    "# Split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess labels with one-hot encoding\n",
    "y_train_one_hot = tf.one_hot(y_train, num_classes).numpy()\n",
    "y_test_one_hot = tf.one_hot(y_test, num_classes).numpy()\n",
    "\n",
    "# Build the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)), # 1st Convolutional layer\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2),padding='same'), # MaxPooling layer\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'), # 2nd Convolutional layer\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=(2,2),padding='same'), # MaxPooling layer\n",
    "    tf.keras.layers.Flatten(), # Flatten the output from the convolutional layers\n",
    "    tf.keras.layers.Dense(128, activation='relu'), # Hidden layer with ReLU activation\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax') # Output layer with softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # Use softmax cross-entropy loss\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1,validation_data=(x_test,y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11bac064",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load your test data (replace this with your actual test data)\n",
    "test_data = test_images\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# The 'predictions' array will contain the predicted class probabilities for each image\n",
    "# You can get the predicted classes by finding the index of the maximum probability\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0852e69b",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.9525\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('tf__convolutional', predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd71a55",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f90e74",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "In transfer learning, we use a network trained on one data set to provide a starting point for the modeling of other data.  As we are trying to model color images, we should look for another network trained on color images.  Luckily, we have already discussed such a network: the Inception network used in the Deep Dream notebook.\n",
    "\n",
    "The following cell will load the model, omitting its classification layer (since we're not interested in classifying `ImageNet` images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91e11a7",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "# include_top=False will discard avg_pool before prediction layer\n",
    "inception = tf.keras.applications.inception_v3.InceptionV3(include_top=True, input_shape=(299, 299, 3))\n",
    "inception = tf.keras.Model([inception.input], [inception.layers[-2].output]) # manually discard prediction layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99995be0",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "In a transfer learning setup, we will use the first part of the trained network to process the original images, and then train a network to make predictions from the output of the trained network.  There are several ways to accomplish this.\n",
    "\n",
    "One approach is to connect the new layers to the output of the existing layers.  Data will flow through the pre-trained layers as well as those added.  In the training step, only the new layers should be marked as trainable.\n",
    "\n",
    "However, this can be a bit wasteful when multiple epochs of training will be undertaken since we recalculate the latent vectors on every training step.  With a smaller data set, such as this, it can be more efficient to pre-calculate the *latent vectors* that are the output of the pre-trained network.  These can be stored and used as input for training a smaller, separate network to make the predictions.  We recommend this approach for this miniproject.\n",
    "\n",
    "Images should be fed to the `inception` network and then vectorized (you might want to refer to the `TF_DeepDream.ipynb` notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef363207",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We've loaded the `inception` network with its native image shape: $299 \\times 299$.  This implies that we need to upscale our images from $32\\times32$ to $299\\times299$.  There are a number of ways to do this; the reference solution uses `tf.image.resize` with bilinear interpolation.  (More sophisticated resizing methods produce better results, but will take significantly longer!)\n",
    "\n",
    "It may make sense to do the rescaling and latent vector calculation at the same time, to avoid storing the (somewhat large) rescaled images unnecessarily.  You also probably want to save those latent vectors to disk, to avoid the need to repeat this calculation later.\n",
    "\n",
    "**Hints:**\n",
    "- Be sure to batch this calculation; resizing all 50,000 images at once will cause memory errors.\n",
    "- The latent vector calculation took us between 30 minutes and 2 hours on a single machine.  You might consider distributing the calculation.\n",
    "- The latent vectors for the first 10 images have an average of 1983 non-zero values and an overall average value of 0.319."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aefc55d",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "With the latent vectors calculated, we can use them as input to train a small neural network to make the final predictions.\n",
    "\n",
    "**Hints:**\n",
    "- The reference solution has three layers.\n",
    "- The reference solution achieves a training accuracy of 87% and a test accuracy of 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45af9024",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 18s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 17s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 17s 2s/step\n",
      "10/10 [==============================] - 20s 2s/step\n",
      "10/10 [==============================] - 17s 2s/step\n",
      "10/10 [==============================] - 17s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "10/10 [==============================] - 16s 2s/step\n",
      "4/4 [==============================] - 6s 1s/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the InceptionV3 model without the classification layer\n",
    "inception = tf.keras.applications.inception_v3.InceptionV3(include_top=True, input_shape=(299, 299, 3))\n",
    "inception = tf.keras.Model(inputs=inception.input, outputs=inception.layers[-2].output)\n",
    "\n",
    "# Load and preprocess your original images (assuming you have a list of images)\n",
    "original_images = train_images[40000:]  # List of original images\n",
    "\n",
    "# Batch size for processing images in batches\n",
    "batch_size = 300\n",
    "\n",
    "# Create a list to store latent vectors\n",
    "latent_vectors = []\n",
    "\n",
    "# Process images in batches\n",
    "for i in range(0, len(original_images), batch_size):\n",
    "    batch_images = original_images[i:i + batch_size]\n",
    "\n",
    "    # Preprocess and compute latent vectors for the batch\n",
    "    batch_latent_vectors = []\n",
    "    for image in batch_images:\n",
    "        resized_image = tf.image.resize(image, (299, 299), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        preprocessed_image = tf.keras.applications.inception_v3.preprocess_input(resized_image)\n",
    "        batch_latent_vectors.append(preprocessed_image)\n",
    "    \n",
    "    # Convert the batch of images to a tensor\n",
    "    batch_latent_vectors = tf.stack(batch_latent_vectors)\n",
    "\n",
    "    # Obtain latent vectors for the batch\n",
    "    batch_latent_vectors = inception.predict(batch_latent_vectors)\n",
    "    latent_vectors.append(batch_latent_vectors)\n",
    "\n",
    "# Concatenate all the batched latent vectors\n",
    "latent_vectors = tf.concat(latent_vectors, axis=0)\n",
    "\n",
    "# Convert latent vectors to a NumPy array\n",
    "latent_vectors_array = latent_vectors.numpy()\n",
    "\n",
    "# Save latent vectors to disk using NumPy's save function\n",
    "np.save('latent_vectors5.npy', latent_vectors_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2d34631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of filenames for saved latent vector arrays\n",
    "latent_vector_filenames = ['latent_vectors1.npy', 'latent_vectors2.npy', 'latent_vectors3.npy', 'latent_vectors4.npy', 'latent_vectors5.npy']\n",
    "\n",
    "# Load each saved array and store them in a list\n",
    "loaded_arrays = []\n",
    "for filename in latent_vector_filenames:\n",
    "    loaded_array = np.load(filename)\n",
    "    loaded_arrays.append(loaded_array)\n",
    "\n",
    "# Concatenate the loaded arrays along axis 0 (row-wise)\n",
    "latent_vectors = np.concatenate(loaded_arrays, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18560697",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "200/200 [==============================] - 12s 54ms/step - loss: 2.3028 - accuracy: 0.1053 - val_loss: 2.2996 - val_accuracy: 0.1040\n",
      "Epoch 2/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 2.2231 - accuracy: 0.1498 - val_loss: 2.1439 - val_accuracy: 0.1809\n",
      "Epoch 3/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 2.1300 - accuracy: 0.1855 - val_loss: 2.0759 - val_accuracy: 0.2124\n",
      "Epoch 4/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 2.0551 - accuracy: 0.2194 - val_loss: 1.9415 - val_accuracy: 0.2690\n",
      "Epoch 5/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.9579 - accuracy: 0.2545 - val_loss: 1.8704 - val_accuracy: 0.2906\n",
      "Epoch 6/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.8961 - accuracy: 0.2802 - val_loss: 1.8514 - val_accuracy: 0.2987\n",
      "Epoch 7/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.8711 - accuracy: 0.2907 - val_loss: 1.7905 - val_accuracy: 0.3248\n",
      "Epoch 8/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.8353 - accuracy: 0.3070 - val_loss: 1.7687 - val_accuracy: 0.3315\n",
      "Epoch 9/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.8208 - accuracy: 0.3154 - val_loss: 1.7824 - val_accuracy: 0.3344\n",
      "Epoch 10/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.7913 - accuracy: 0.3239 - val_loss: 1.7121 - val_accuracy: 0.3609\n",
      "Epoch 11/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.7853 - accuracy: 0.3284 - val_loss: 1.7112 - val_accuracy: 0.3599\n",
      "Epoch 12/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.7701 - accuracy: 0.3352 - val_loss: 1.7319 - val_accuracy: 0.3422\n",
      "Epoch 13/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.7638 - accuracy: 0.3390 - val_loss: 1.7024 - val_accuracy: 0.3615\n",
      "Epoch 14/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.7521 - accuracy: 0.3438 - val_loss: 1.6840 - val_accuracy: 0.3652\n",
      "Epoch 15/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.7366 - accuracy: 0.3509 - val_loss: 1.6797 - val_accuracy: 0.3679\n",
      "Epoch 16/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.7282 - accuracy: 0.3563 - val_loss: 1.6927 - val_accuracy: 0.3722\n",
      "Epoch 17/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.7168 - accuracy: 0.3626 - val_loss: 1.6727 - val_accuracy: 0.3841\n",
      "Epoch 18/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.7097 - accuracy: 0.3672 - val_loss: 1.6330 - val_accuracy: 0.3952\n",
      "Epoch 19/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.7113 - accuracy: 0.3675 - val_loss: 1.6232 - val_accuracy: 0.4041\n",
      "Epoch 20/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6953 - accuracy: 0.3781 - val_loss: 1.6124 - val_accuracy: 0.4064\n",
      "Epoch 21/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6903 - accuracy: 0.3742 - val_loss: 1.6650 - val_accuracy: 0.3754\n",
      "Epoch 22/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6893 - accuracy: 0.3764 - val_loss: 1.5948 - val_accuracy: 0.4122\n",
      "Epoch 23/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6743 - accuracy: 0.3849 - val_loss: 1.6056 - val_accuracy: 0.4133\n",
      "Epoch 24/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6750 - accuracy: 0.3824 - val_loss: 1.6035 - val_accuracy: 0.4107\n",
      "Epoch 25/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6668 - accuracy: 0.3915 - val_loss: 1.5920 - val_accuracy: 0.4179\n",
      "Epoch 26/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6610 - accuracy: 0.3955 - val_loss: 1.5741 - val_accuracy: 0.4226\n",
      "Epoch 27/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6537 - accuracy: 0.3961 - val_loss: 1.5975 - val_accuracy: 0.4036\n",
      "Epoch 28/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.6405 - accuracy: 0.4031 - val_loss: 1.5550 - val_accuracy: 0.4339\n",
      "Epoch 29/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.6565 - accuracy: 0.3968 - val_loss: 1.5753 - val_accuracy: 0.4232\n",
      "Epoch 30/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.6397 - accuracy: 0.4071 - val_loss: 1.5671 - val_accuracy: 0.4272\n",
      "Epoch 31/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6364 - accuracy: 0.4065 - val_loss: 1.5722 - val_accuracy: 0.4214\n",
      "Epoch 32/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6281 - accuracy: 0.4119 - val_loss: 1.5754 - val_accuracy: 0.4278\n",
      "Epoch 33/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6228 - accuracy: 0.4110 - val_loss: 1.5351 - val_accuracy: 0.4444\n",
      "Epoch 34/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6250 - accuracy: 0.4122 - val_loss: 1.5347 - val_accuracy: 0.4432\n",
      "Epoch 35/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6105 - accuracy: 0.4168 - val_loss: 1.5354 - val_accuracy: 0.4346\n",
      "Epoch 36/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.6115 - accuracy: 0.4202 - val_loss: 1.5250 - val_accuracy: 0.4457\n",
      "Epoch 37/40\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.6133 - accuracy: 0.4188 - val_loss: 1.5478 - val_accuracy: 0.4457\n",
      "Epoch 38/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.6080 - accuracy: 0.4189 - val_loss: 1.5276 - val_accuracy: 0.4406\n",
      "Epoch 39/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.6035 - accuracy: 0.4216 - val_loss: 1.5516 - val_accuracy: 0.4328\n",
      "Epoch 40/40\n",
      "200/200 [==============================] - 11s 54ms/step - loss: 1.5924 - accuracy: 0.4261 - val_loss: 1.5341 - val_accuracy: 0.4412\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.5341 - accuracy: 0.4412\n",
      "Test accuracy: 0.44119998812675476\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_classes = 10\n",
    "# Load your labels (assuming you have a list of labels corresponding to the original images)\n",
    "labels = train_labels\n",
    "\n",
    "# Split the data into training and validation sets using train_test_split\n",
    "train_latent_vectors, val_latent_vectors, train_labels, val_labels = train_test_split(\n",
    "    latent_vectors, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define your prediction network\n",
    "prediction_model = models.Sequential([\n",
    "    layers.Input(shape=(latent_vectors.shape[1],)),\n",
    "    layers.Reshape((32, 32, 2)),  # Reshape latent vectors to match image dimensions\n",
    "\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "prediction_model.compile(optimizer='adam',\n",
    "                         loss='sparse_categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Train the prediction model\n",
    "history = prediction_model.fit(train_latent_vectors, train_labels,\n",
    "                               epochs=40,\n",
    "                               batch_size=200,\n",
    "                               validation_data=(val_latent_vectors, val_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = prediction_model.evaluate(val_latent_vectors, val_labels)\n",
    "print(\"Test accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04cb756c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 [==============================] - 11s 100ms/step - loss: 1.5717 - accuracy: 0.4365 - val_loss: 1.4824 - val_accuracy: 0.4662\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 1.5592 - accuracy: 0.4395 - val_loss: 1.5253 - val_accuracy: 0.4539\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 1.5563 - accuracy: 0.4414 - val_loss: 1.4814 - val_accuracy: 0.4619\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5598 - accuracy: 0.4399 - val_loss: 1.4936 - val_accuracy: 0.4553\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 1.5491 - accuracy: 0.4457 - val_loss: 1.4744 - val_accuracy: 0.4677\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5557 - accuracy: 0.4422 - val_loss: 1.5070 - val_accuracy: 0.4592\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5518 - accuracy: 0.4444 - val_loss: 1.4974 - val_accuracy: 0.4576\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5537 - accuracy: 0.4464 - val_loss: 1.4768 - val_accuracy: 0.4698\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5437 - accuracy: 0.4469 - val_loss: 1.4842 - val_accuracy: 0.4631\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 1.5372 - accuracy: 0.4497 - val_loss: 1.4885 - val_accuracy: 0.4611\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 1.5376 - accuracy: 0.4500 - val_loss: 1.4783 - val_accuracy: 0.4687\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 1.5407 - accuracy: 0.4505 - val_loss: 1.4741 - val_accuracy: 0.4651\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 1.5345 - accuracy: 0.4514 - val_loss: 1.4631 - val_accuracy: 0.4706\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5219 - accuracy: 0.4558 - val_loss: 1.5050 - val_accuracy: 0.4598\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5266 - accuracy: 0.4562 - val_loss: 1.4837 - val_accuracy: 0.4692\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5237 - accuracy: 0.4551 - val_loss: 1.4740 - val_accuracy: 0.4705\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5279 - accuracy: 0.4518 - val_loss: 1.4753 - val_accuracy: 0.4736\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5367 - accuracy: 0.4505 - val_loss: 1.4641 - val_accuracy: 0.4750\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5156 - accuracy: 0.4609 - val_loss: 1.4375 - val_accuracy: 0.4805\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5192 - accuracy: 0.4576 - val_loss: 1.4543 - val_accuracy: 0.4743\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5119 - accuracy: 0.4627 - val_loss: 1.4465 - val_accuracy: 0.4795\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5171 - accuracy: 0.4589 - val_loss: 1.4584 - val_accuracy: 0.4785\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5041 - accuracy: 0.4621 - val_loss: 1.4445 - val_accuracy: 0.4870\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 1.5040 - accuracy: 0.4627 - val_loss: 1.4422 - val_accuracy: 0.4775\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 1.5053 - accuracy: 0.4642 - val_loss: 1.4473 - val_accuracy: 0.4814\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 1.4991 - accuracy: 0.4660 - val_loss: 1.4367 - val_accuracy: 0.4860\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 1.5075 - accuracy: 0.4616 - val_loss: 1.4671 - val_accuracy: 0.4704\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4920 - accuracy: 0.4680 - val_loss: 1.4246 - val_accuracy: 0.4869\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.5033 - accuracy: 0.4637 - val_loss: 1.4144 - val_accuracy: 0.4864\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4923 - accuracy: 0.4682 - val_loss: 1.4229 - val_accuracy: 0.4847\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4896 - accuracy: 0.4708 - val_loss: 1.4208 - val_accuracy: 0.4873\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 1.4834 - accuracy: 0.4714 - val_loss: 1.4090 - val_accuracy: 0.4954\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4809 - accuracy: 0.4741 - val_loss: 1.4210 - val_accuracy: 0.4855\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 1.4802 - accuracy: 0.4738 - val_loss: 1.4424 - val_accuracy: 0.4809\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4817 - accuracy: 0.4742 - val_loss: 1.4362 - val_accuracy: 0.4714\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.4812 - accuracy: 0.4741 - val_loss: 1.4392 - val_accuracy: 0.4825\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 1.4835 - accuracy: 0.4725 - val_loss: 1.4237 - val_accuracy: 0.4869\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4770 - accuracy: 0.4746 - val_loss: 1.3975 - val_accuracy: 0.4945\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4706 - accuracy: 0.4764 - val_loss: 1.4103 - val_accuracy: 0.4908\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4634 - accuracy: 0.4791 - val_loss: 1.4098 - val_accuracy: 0.4929\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 1.4671 - accuracy: 0.4763 - val_loss: 1.3970 - val_accuracy: 0.5002\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4622 - accuracy: 0.4806 - val_loss: 1.4016 - val_accuracy: 0.4941\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 1.4614 - accuracy: 0.4797 - val_loss: 1.3928 - val_accuracy: 0.4991\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4541 - accuracy: 0.4855 - val_loss: 1.3851 - val_accuracy: 0.4971\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 1.4615 - accuracy: 0.4799 - val_loss: 1.4121 - val_accuracy: 0.4918\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4614 - accuracy: 0.4793 - val_loss: 1.4023 - val_accuracy: 0.4908\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4510 - accuracy: 0.4846 - val_loss: 1.3919 - val_accuracy: 0.5008\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 1.4537 - accuracy: 0.4818 - val_loss: 1.3880 - val_accuracy: 0.5026\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4446 - accuracy: 0.4880 - val_loss: 1.4185 - val_accuracy: 0.4824\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 1.4550 - accuracy: 0.4815 - val_loss: 1.3933 - val_accuracy: 0.4988\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.3933 - accuracy: 0.4988\n"
     ]
    }
   ],
   "source": [
    "# Train the prediction model\n",
    "history = prediction_model.fit(train_latent_vectors, train_labels,\n",
    "                               epochs=50,\n",
    "                               batch_size=400,\n",
    "                               validation_data=(val_latent_vectors, val_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = prediction_model.evaluate(val_latent_vectors, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a3d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load your test data\n",
    "test_data = val_latent_vectors\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = prediction_model.predict(test_data)\n",
    "\n",
    "# The 'predictions' array will contain the predicted class probabilities for each image\n",
    "# You can get the predicted classes by finding the index of the maximum probability\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1334165a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 26s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "i=0\n",
    "\n",
    "while (i != 20):\n",
    "\n",
    "    test_images_resized = [tf.image.resize(\n",
    "                            test_images[(i)*500:(i+1)*500],\n",
    "                            (299,299),\n",
    "                            method = 'bilinear')]\n",
    "    \n",
    "    bravo = inception.predict(test_images_resized)\n",
    "    np.save(f'test_image_batch_{i}',bravo)\n",
    "    \n",
    "    i = i + 1\n",
    "    display(i)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac546743",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "\n",
    "while (i != 100):\n",
    "\n",
    "    train_images_resized = [tf.image.resize(\n",
    "                            train_images[(i)*500:(i+1)*500],\n",
    "                            (299,299),\n",
    "                            method = 'bilinear')]\n",
    "    \n",
    "    bravo = inception.predict(train_images_resized)\n",
    "    np.save(f'train_image_batch_{i}',bravo)\n",
    "    \n",
    "    i = i + 1\n",
    "    display(i)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb3979bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical as one_hot\n",
    "train_labels_onehot = one_hot(train_labels, num_classes=10)\n",
    "\n",
    "new_model = tf.keras.models.Sequential()\n",
    "new_model.add(tf.keras.layers.Dense(1024,activation='relu'))\n",
    "new_model.add(tf.keras.layers.Dropout(0.3))\n",
    "new_model.add(tf.keras.layers.Dense(1024,activation='relu'))\n",
    "new_model.add(tf.keras.layers.Dropout(0.3))\n",
    "new_model.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
    "\n",
    "new_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=tf.keras.optimizers.SGD(),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "load4train = np.load('train_image_batch_0.npy')\n",
    "\n",
    "for i in range(1,100):\n",
    "    load_next = np.load(f'train_image_batch_{i}.npy')\n",
    "    load4train = np.concatenate((load4train,load_next), axis=0)\n",
    "    \n",
    "load4test = np.load('test_image_batch_0.npy')\n",
    "\n",
    "for i in range(1,20):\n",
    "    load_next_test = np.load(f'test_image_batch_{i}.npy')\n",
    "    load4test = np.concatenate((load4test,load_next_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4929af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the labels into training and testing sets\n",
    "train_labels, test_labels = train_test_split(train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "test_labels_onehot = one_hot(test_labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a32ac318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load4train shape: (50000, 2048)\n",
      "train_labels_onehot shape: (50000, 10)\n",
      "load4test shape: (10000, 2048)\n",
      "test_labels shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"load4train shape:\", load4train.shape)\n",
    "print(\"train_labels_onehot shape:\", train_labels_onehot.shape)\n",
    "print(\"load4test shape:\", load4test.shape)\n",
    "print(\"test_labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66c09cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2048)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load4test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de960d31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 7s 27ms/step - loss: 1.3310 - accuracy: 0.5727 - val_loss: 4.1577 - val_accuracy: 0.0944\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.7988 - accuracy: 0.7317 - val_loss: 5.0293 - val_accuracy: 0.0952\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.6953 - accuracy: 0.7651 - val_loss: 5.4287 - val_accuracy: 0.0959\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.6452 - accuracy: 0.7810 - val_loss: 5.6897 - val_accuracy: 0.0956\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.6121 - accuracy: 0.7904 - val_loss: 5.8256 - val_accuracy: 0.0963\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.5880 - accuracy: 0.7994 - val_loss: 5.9490 - val_accuracy: 0.0953\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 6s 25ms/step - loss: 0.5671 - accuracy: 0.8052 - val_loss: 6.0882 - val_accuracy: 0.0954\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.5497 - accuracy: 0.8115 - val_loss: 6.1889 - val_accuracy: 0.0949\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.5386 - accuracy: 0.8141 - val_loss: 6.2246 - val_accuracy: 0.0955\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 6s 25ms/step - loss: 0.5232 - accuracy: 0.8204 - val_loss: 6.3594 - val_accuracy: 0.0960\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.5133 - accuracy: 0.8227 - val_loss: 6.4278 - val_accuracy: 0.0955\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.5017 - accuracy: 0.8279 - val_loss: 6.5329 - val_accuracy: 0.0962\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.4935 - accuracy: 0.8312 - val_loss: 6.5688 - val_accuracy: 0.0949\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.4884 - accuracy: 0.8313 - val_loss: 6.6366 - val_accuracy: 0.0956\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 6s 25ms/step - loss: 0.4808 - accuracy: 0.8340 - val_loss: 6.6548 - val_accuracy: 0.0951\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 6s 25ms/step - loss: 0.4714 - accuracy: 0.8362 - val_loss: 6.7482 - val_accuracy: 0.0961\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.4649 - accuracy: 0.8396 - val_loss: 6.8153 - val_accuracy: 0.0953\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.4582 - accuracy: 0.8421 - val_loss: 6.8585 - val_accuracy: 0.0961\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 7s 26ms/step - loss: 0.4504 - accuracy: 0.8445 - val_loss: 6.9644 - val_accuracy: 0.0963\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 6s 26ms/step - loss: 0.4484 - accuracy: 0.8445 - val_loss: 6.9466 - val_accuracy: 0.0959\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 6.9466 - accuracy: 0.0959\n",
      "Test accuracy: 0.09589999914169312\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode validation labels\n",
    "test_labels_onehot = one_hot(test_labels, num_classes=10)  # Use the same num_classes as for training\n",
    "\n",
    "# Train the prediction model\n",
    "new_model.fit(load4train, train_labels_onehot,\n",
    "              epochs=20,\n",
    "              batch_size=200,\n",
    "              validation_data=(load4test, test_labels_onehot))  # Use test_labels_onehot here\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = new_model.evaluate(load4test, test_labels_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dcbefe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = new_model.predict([load4test])\n",
    "submitter=[]\n",
    "for prediction in predictions:\n",
    "    submitter.append(np.argmax(prediction))\n",
    "\n",
    "# The 'predictions' array will contain the predicted class probabilities for each image\n",
    "# You can get the predicted classes by finding the index of the maximum probability\n",
    "#predicted_classes = np.argmax(predictions, axis=1)\n",
    "submitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83671029",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0045\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#predicted_classes = [0]*len(test_images)\n",
    "\n",
    "grader.score('tf__transfer_learning', submitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6fd99",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2023 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
