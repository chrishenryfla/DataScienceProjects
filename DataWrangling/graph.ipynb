{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9be088",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1233767",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from static_grader import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b4db6",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# The New York Social Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02b8f7",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "[New York Social Diary](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/) provides a\n",
    "fascinating lens onto New York's socially well-to-do.  The data forms a natural [social graph](https://en.wikipedia.org/wiki/Social_graph) for New York's social elite.  Take a look at this page of a recent [run-of-the-mill holiday party](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures/2014/holiday-dinners-and-doers).\n",
    "\n",
    "Besides the brand-name celebrities, you will notice the photos have carefully annotated captions labeling those that appear in the photos.  We can think of this as implicitly implying a social graph: there is a connection between two individuals if they appear in a picture together.\n",
    "\n",
    "For this project, we will assemble the social graph from photo captions for parties _dated December 1, 2014 and before_.  Using this graph, we can make guesses at the most popular socialites, the most influential people, and the most tightly coupled pairs.\n",
    "\n",
    "These pages are hosted on the Internet Archive, which can be quite slow and unreliable. To get around this, we have created an API that provides the captions. This API lives at `https://party-captions.tditrain.com`. The [documentation](https://party-captions.tditrain.com) describes how this API works in detail. At a high level, it's divided into two parts\n",
    "- An endpoint that provides a list of parties, `/parties`\n",
    "- An endpoint that provides the captions for a given party, `/captions`\n",
    "\n",
    "Both take parameters that allow us to select what we're looking for.\n",
    "\n",
    "To get the social graph that we want, we'll attack the problem in several steps:\n",
    "1. Get a list of the parties we want\n",
    "1. Parse the names from each caption for one party\n",
    "1. Parse the names for the rest of the parties\n",
    "1. Assemble the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca67250",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Getting the parties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e0b4d",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The `/parties` endpoint provides us with a list of party names and dates (the date the party occurred). It can only provide up to 100 at a time, and there are over 1000 parties in the data set. By using the `limit` and `offset` parameters, as described in the documentation, get a list of all of the parties and their dates.\n",
    "\n",
    "As we did in class, we recommend using [`requests`](http://docs.python-requests.org/en/master/) to hit the endpoint. The checkpoints are expecting a list where each element corresponds to one party. How you want to represent this party (as a tuple, a dictionary, or something else) is up to you.\n",
    "\n",
    "The API will return a JSON object containing a list of party names and dates, and some metadata.  Here is an example, only returning the first ten parties for our convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022ea0f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We want the information in the `parties` element. You will need to call the API multiple times to get all the parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc32f5cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://party-captions.tditrain.com/parties?limit=100&offset=0\n",
      "https://party-captions.tditrain.com/parties?limit=100&offset=100\n",
      "https://party-captions.tditrain.com/parties?limit=100&offset=200\n",
      "https://party-captions.tditrain.com/parties?limit=100&offset=300\n",
      "https://party-captions.tditrain.com/parties?limit=100&offset=400\n",
      "https://party-captions.tditrain.com/parties?limit=100&offset=500\n",
      "https://party-captions.tditrain.com/parties?limit=100&offset=600\n",
      "https://party-captions.tditrain.com/parties?limit=100&offset=700\n",
      "https://party-captions.tditrain.com/parties?limit=100&offset=800\n",
      "https://party-captions.tditrain.com/parties?limit=100&offset=900\n",
      "https://party-captions.tditrain.com/parties?limit=100&offset=1000\n",
      "https://party-captions.tditrain.com/parties?limit=100&offset=1100\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "offset_val=[*range(0, 1200, 100)]\n",
    "listy=[]\n",
    "\n",
    "for number in offset_val:\n",
    "    print(f\"https://party-captions.tditrain.com/parties?limit=100&offset={number}\")\n",
    "    urls= f\"https://party-captions.tditrain.com/parties?limit=100&offset={number}\"\n",
    "    newlisty=requests.get(urls).json()\n",
    "    newlisty=newlisty.get(\"parties\")\n",
    "    listy.append(newlisty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab168115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "date_string = '1 December, 2014'\n",
    "obj = datetime.strptime(date_string, \"%d %B,  %Y\")\n",
    "obj\n",
    "\n",
    "####for loop\n",
    "\n",
    "party_list = []\n",
    "for i in range(len(listy)):\n",
    "    dataList = listy[i]\n",
    "    \n",
    "    for j in range(len(dataList)):\n",
    "        date_test = dataList[j]['date']\n",
    "        date_test\n",
    "\n",
    "        obj2 = datetime.strptime(date_test, \"%Y-%m-%d\")\n",
    "        \n",
    "\n",
    "        if obj >= obj2:\n",
    "            #save to partylist then append\n",
    "            party_list.append(listy[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a50e5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#party_list\n",
    "import re\n",
    "party_list777 = ', '.join(d['name'] for d in party_list)\n",
    "party_list777 = re.split(\", \",party_list777) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b0ffb",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now that we have our list of parties, we'll need to remove those that occurred after December 1st, 2014 (we keep the ones that occurred _on_ or before that date). The API provided us with the dates, as strings. One option would be to use `datetime`'s `strptime` method and the [format codes for dates](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) to parse this into dates for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0577d7b6",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "party_list = party_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f3b95f9",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have successfully gotten all of the parties, there should be 1145 of them\n",
    "# Double check that you are not skipping or duplicating parties\n",
    "# if you are, look at how you are incrementing your offset\n",
    "# Have you filtered by the date?\n",
    "grader.check(len(party_list) == 1145)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1395d0c3",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "To avoid having to get the party list from the API again if we restart the notebook, we should save this list to a file. There are many ways to do it, here's how with `dill`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749ed66",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open('nysd-parties.pkd', 'wb') as f:\n",
    "    dill.dump(party_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3045ba",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "And to read it back later, we just `load` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fce5d594",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('nysd-parties.pkd', 'rb') as f:\n",
    "    party_list = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc795332",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 1: histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea826f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Get the number of party pages for each of the 95 months (that is, month-year pair) in the data.  Represent this histogram as a list of 95 tuples, each of the form `(\"Dec-2014\", 1)`.  Note that you can convert `datetime` objects into these sort of strings with `strftime` and the [format codes for dates](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) from before.\n",
    "\n",
    "The grader is expecting the list of tuples. \n",
    "\n",
    "Plot the histogram for yourself.  Do you see any trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d8efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50893049",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_dates = [datetime.strftime(datetime.strptime(p['date'], \"%Y-%m-%d\"), \"%b-%Y\")  for p in party_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6a03f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn counter object into items/list\n",
    "party_dates = list(Counter(party_dates).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a5a217f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(party_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fecc37e1",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "histogram = party_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba338eb",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('graph__histogram', histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2509d",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Parsing captions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba425b3",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We now have all of the parties.  For each party, we'll need to get the captions, then find who appears in each caption. Let's start with a single party, [the benefit cocktails and dinner](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood) for [Lenox Hill Neighborhood House](http://www.lenoxhill.org/), a neighborhood organization for the East Side. In our API, this corresponds to the party named `2015-celebrating-the-neighborhood`.  Let's get the captions for it.\n",
    "\n",
    "In our API, the `/captions` endpoint takes a parameter `party`, which we give the name of the party we want. We can then extract the captions from the JSON it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba4e6c5f",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "response = requests.get('https://party-captions.tditrain.com/captions?party=2015-celebrating-the-neighborhood')\n",
    "captions = response.json()['captions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae2b5c2",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# We'll need to do this for all of our parties later, so we should make it a function we can call. Take the party name as an argument and return the list of captions. \n",
    "\n",
    "We want to avoid having to hit the API repeatedly the next time we need to run the notebook.  While you could save the files by hand, as we did before, a checkpoint library like [ediblepickle](https://pypi.python.org/pypi/ediblepickle/1.1.3) can handle this for you.  (Note, though, that you may not want to enable this until you are sure that your function is working.)\n",
    "\n",
    "You should also keep in mind that HTTP requests fail occasionally, for transient reasons.  You should plan how to detect and react to these failures.   The [retrying module](https://pypi.python.org/pypi/retrying) is one way to deal with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fba86a8",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "def get_captions(party_name):\n",
    "    return requests.get('https://party-captions.tditrain.com/captions?party=' + party_name).json()['captions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9e948",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "If things have gone according to plan, this should get the same captions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119326b9",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell is expecting get_captions to return a list of the captions themselves\n",
    "# Other routes to a solution might need to adjust this cell a bit\n",
    "grader.check(get_captions('2015-celebrating-the-neighborhood') == captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4db2198d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Glenn Adamson, Simon Doonan, Victoire de Castellane, Craig Leavitt, Jerome Chazen, Andi Potamkin, Ralph Pucci, Kirsten Bailey, Edwin Hathaway, and Dennis Freedman at the Museum of Art and Design's annual MAD BALL. \",\n",
       " ' Randy Takian ',\n",
       " ' Kamie Lightburn and Christopher Spitzmiller ',\n",
       " ' Christopher Spitzmiller and Diana Quasha ',\n",
       " ' Mariam Azarm, Sana Sabbagh, and Lynette Dallas ',\n",
       " ' Christopher Spitzmiller, Sydney Shuman, and Matthew Bees',\n",
       " ' Christopher Spitzmiller and Tom Edelman ',\n",
       " ' Warren Scharf and Sydney Shuman ',\n",
       " ' Amory McAndrew and Sean McAndrew ',\n",
       " ' Sydney Shuman, Mario Buatta, and Helene Tilney ',\n",
       " ' Katherine DeConti and Elijah Duckworth-Schachter ',\n",
       " ' John Rosselli and Elizabeth Swartz ',\n",
       " ' Stephen Simcock, Lee Strock, and Thomas Hammer ',\n",
       " ' Searcy Dryden, Lesley Dryden, Richard Lightburn, and Michel Witmer ',\n",
       " ' Jennifer Cacioppo and Kevin Michael Barba ',\n",
       " ' Virginia Wilbanks and Lacary Sharpe ',\n",
       " ' Valentin Hernandez, Yaz Hernandez, Chele Farley, and James Farley',\n",
       " ' Harry Heissmann, Angela Clofine, and Michael Clofine',\n",
       " ' Jared Goss and Kristina Stewart Ward ',\n",
       " ' Alex Papachristidis and Mario Buatta ',\n",
       " ' Nick Olsen, Lindsey Coral Harper, Alberto Villalobos, and David Duncan ',\n",
       " ' Caroline Dean, Christopher Spitzmiller, and Ellen Niven ',\n",
       " ' Debbie Bancroft and David Svanda ',\n",
       " ' Julia Weld and Christopher Spitzmiller ',\n",
       " ' Kevin Lichten, Robert Ruffino, Joan Craig, and Michael McGraw ',\n",
       " ' Roric Tobin, Chele Farley, Richard Farley, and Geoffrey Bradfield',\n",
       " ' Lacary Sharpe, Kamie Lightburn, and Emily Leonard ',\n",
       " ' Dr. Doug Steinbrech and Mary Van Pelt ',\n",
       " ' Kathy  and Othon Prounis ',\n",
       " 'The MAD dinner scene',\n",
       " 'Victoire de Castellane, Dennis Freedman, Andi Potamkin, and Ralph Pucci',\n",
       " 'Donald Tober, Barbara Tober, Linda Fargo, and Bjorn Wallander',\n",
       " 'Andi Potamkin, Glenn Adamson, Ralph Pucci, Victoire de Castellane, Dennis Freedman, and Craig Leavitt',\n",
       " 'Celia Morrissette and Keith Johnson',\n",
       " 'Simon Doonan',\n",
       " 'Sandra Gering and Julia Kuni',\n",
       " 'Alexandra Richards and Paul Longo',\n",
       " 'Bryna and Martin Pomp',\n",
       " 'Neville Wakefield',\n",
       " 'Caroline Cokley',\n",
       " 'Ryan Davis',\n",
       " 'Freddie Leiba and Cynthia Adler',\n",
       " 'Rick and Leticia Presutti',\n",
       " ' Glenn Adamson and Ebony G. Patterson',\n",
       " 'Lowery Stokes Sims and C. Virginia Fields',\n",
       " 'Barbara Regna',\n",
       " 'Deborah Lloyd',\n",
       " 'Kelly  and Jay Sugarman',\n",
       " 'Steve Evans, Kate White, Liz Nacey, Anne Strickland, Patrick Jones, and Jennifer Fujitani',\n",
       " 'Wendy Diamond',\n",
       " 'Tood Eberle and Gina Nanni',\n",
       " 'Linda Fargo and Bjorn Wallander',\n",
       " 'Tony Ingrao and Randy Kemper',\n",
       " 'C. Virginia Fields, Gordan Kenney, and Jill Huggins',\n",
       " 'Wendy MacGaw and Howard Ben Tre',\n",
       " 'Laura  and Lewis Kruger',\n",
       " ' The Waldorf Astoria Ballroom ',\n",
       " ' Deborah Marton (Executive Director of the NYRP) ',\n",
       " ' Bette Midler ',\n",
       " ' Lance LePere and Michael Kors ',\n",
       " 'A table centerpiece',\n",
       " \" Linda Allard's party \",\n",
       " ' Mrs. and Mrs. Ben Needell Esq. ',\n",
       " ' Nile Rodgers and Bette Midler ',\n",
       " ' Todd DeGarmo and Deborah Marton ',\n",
       " ' Samuel Kelly ',\n",
       " ' David and Lola Rockwell ',\n",
       " ' Davon Windsor and Rachel Hilbert ',\n",
       " 'Hula girls welcome the guests',\n",
       " ' Honorees Sheryl and and Dan Tishman with Bette Midler',\n",
       " ' Darcy Stacom ',\n",
       " ' Judy Gold and Bette Midler ',\n",
       " ' Kevin and Michelle Fox',\n",
       " ' Jane Krakowski, Michael Kors, Bette Midler, and Lance LePere ',\n",
       " '\\n\\n\\n Shoshanna Gruss and Dr. Drew Schiff \\n\\n\\n',\n",
       " '\\n\\n\\n Margo and Jimmy Nederlander\\n\\n\\n\\n',\n",
       " '\\n\\n\\n Nederlander table as Zombie Gillian’s Island \\n\\n\\n',\n",
       " '\\n\\n\\n Jon Recor\\n\\n\\n\\n',\n",
       " '\\n\\n\\n Shelly Malkin and Nathalie Gerschel Kaplan \\n\\n\\n',\n",
       " ' Zombie dancers in silver corridor ',\n",
       " ' Douglas Little and friend',\n",
       " ' Zombie airplane staff directing guests around the party ',\n",
       " ' Martin von Haselberg ',\n",
       " ' Mr. and Mrs. Allen Swerdlick ',\n",
       " \" Ta'Rhonda Jones of Empire (on right) \",\n",
       " ' Lance LePere, Jane Krakowski, and Michael Kors ',\n",
       " ' Kelly Bensimon and John Demsey',\n",
       " ' Michael Kors and Lauren duPont ',\n",
       " ' Nancy Hunt, Bette Midler, and Nile Rodgers ',\n",
       " ' Nile Rodgers & CHIC with Bette Midler on stage ',\n",
       " ' Thriller zombie dancers ',\n",
       " 'Richard and Donna Soloway',\n",
       " 'Barbara  and Peter Regna',\n",
       " 'Jonas Barcellos and friends',\n",
       " 'Michelle Halpern, Mark Robinson, and Lisa Halpern',\n",
       " 'Clark  and Esra Munnell',\n",
       " 'Elizabeth Johnson and friends',\n",
       " 'Wendy Carduner and George Perry',\n",
       " 'Lisa and Tom Wilkenson',\n",
       " ' Stacy Reilly and friends',\n",
       " 'Eveyln Subramaniam',\n",
       " 'Sarah and Chips Page',\n",
       " 'Mrs. DeMaurier and friend',\n",
       " 'Jim Killerlane and Allison Minton',\n",
       " 'Liana Makkos',\n",
       " 'Ken and Maria Fishel, Tina Wong, and George Perry',\n",
       " 'Marisa Rose and \"KISS\" friends',\n",
       " 'Eveyln Subramaniam',\n",
       " 'Alexis Mersentes and Doris Liebman']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testlist = get_captions('2015-celebrating-the-neighborhood')\n",
    "testlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52dc6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def extract_names(party_name):\n",
    "    names = []\n",
    "    capshun_list = get_captions(party_name)\n",
    "    for caption in capshun_list:\n",
    "        words = word_tokenize(caption)\n",
    "        tagged_words = pos_tag(words)\n",
    "        for i in range(len(tagged_words)):\n",
    "            if tagged_words[i][1] == 'NNP':  # NNP stands for proper noun\n",
    "                name = tagged_words[i][0]\n",
    "                j = i + 1\n",
    "                while j < len(tagged_words) and tagged_words[j][1] == 'NNP':\n",
    "                    name += ' ' + tagged_words[j][0]\n",
    "                    j += 1\n",
    "                if re.match(r'^[A-Z][a-z]*\\s[A-Z][a-z]*$', name):  # match full names only\n",
    "                    names.append(name)\n",
    "    return names\n",
    "jj = extract_names('2015-celebrating-the-neighborhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "57d811fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative route for name parsing using nltk outputting a tuple with assocciating values for each caption\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def extract_names_as_tuple(party_name):\n",
    "    names = []\n",
    "    capshun_list = get_captions(party_name)\n",
    "    for index, caption in enumerate(captions):\n",
    "        words = word_tokenize(caption)\n",
    "        tagged_words = pos_tag(words)\n",
    "        for i in range(len(tagged_words)):\n",
    "            if tagged_words[i][1] == 'NNP':  # NNP stands for proper noun\n",
    "                name = tagged_words[i][0]\n",
    "                j = i + 1\n",
    "                while j < len(tagged_words) and tagged_words[j][1] == 'NNP':\n",
    "                    name += ' ' + tagged_words[j][0]\n",
    "                    j += 1\n",
    "                if re.match(r'^[A-Z][a-z]*\\s[A-Z][a-z]*$', name):  # match full names only\n",
    "                    names.append((index, name))\n",
    "    namesonly_ = [element for sublist in names for element in sublist]\n",
    "    return names,namesonly_\n",
    "#jj = extract_names_as_tuple(testlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "947e2e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking in a list of tuples, joining the names found in each caption to return a list of lists\n",
    "def join_names(input_names):\n",
    "    name_dict = {}\n",
    "    for index, name in input_names:\n",
    "        if index in name_dict:\n",
    "            name_dict[index].append(name)\n",
    "        else:\n",
    "            name_dict[index] = [name]\n",
    "    output_names = list(name_dict.values())\n",
    "    return output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d85c7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined functions\n",
    "def extract_and_join_names(party_name):\n",
    "    names = []\n",
    "    capshun_list = get_captions(party_name)\n",
    "    for index, caption in enumerate(capshun_list):\n",
    "        words = word_tokenize(caption)\n",
    "        tagged_words = pos_tag(words)\n",
    "        for i in range(len(tagged_words)):\n",
    "            if tagged_words[i][1] == 'NNP':  # NNP stands for proper noun\n",
    "                name = tagged_words[i][0]\n",
    "                j = i + 1\n",
    "                while j < len(tagged_words) and tagged_words[j][1] == 'NNP':\n",
    "                    name += ' ' + tagged_words[j][0]\n",
    "                    j += 1\n",
    "                if re.match(r'^[A-Z][a-z]*\\s[A-Z][a-z]*$', name):  # match full names only\n",
    "                    names.append((index, name))\n",
    "    \n",
    "    name_dict = {}\n",
    "    for index, name in names:\n",
    "        if index in name_dict:\n",
    "            name_dict[index].append(name)\n",
    "        else:\n",
    "            name_dict[index] = [name]\n",
    "    \n",
    "    output_names = list(name_dict.values())\n",
    "   \n",
    "    return output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3b555502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Fujiko Nakaya', 'Glass House'],\n",
       " ['Mary Lattimore'],\n",
       " ['Charles Renfro', 'Tony Vidler', 'James Welling', 'Daniel Gortler'],\n",
       " ['Margaret Russell', 'Henry Urbach', 'Margareth Henriquez'],\n",
       " ['Carlos Souza'],\n",
       " ['Bonnie Morrison', 'Douglas Friedman'],\n",
       " ['Painting Gallery'],\n",
       " ['Martha Stewart', 'Charles Renfro'],\n",
       " ['Margaret Russell',\n",
       "  'Martha Stewart',\n",
       "  'Magrino Dunning',\n",
       "  'Margareth Henriquez',\n",
       "  'Reed Krakoff',\n",
       "  'John Yunis',\n",
       "  'John Calcagno'],\n",
       " ['Lee Mindel',\n",
       "  'Margaret Russell',\n",
       "  'Tomas Maier',\n",
       "  'Martha Stewart',\n",
       "  'Magrino Dunning'],\n",
       " ['Bill Katz',\n",
       "  'Bonnie Morrison',\n",
       "  'Douglas Friedman',\n",
       "  'Henry Urbach',\n",
       "  'Laurie Beckelman',\n",
       "  'James Sanders',\n",
       "  'Tony Vidler',\n",
       "  'Charles Renfro'],\n",
       " ['Rich Kosann', 'Reed Krakoff'],\n",
       " ['Hendel Teicher'],\n",
       " ['Henry Urbach', 'Charles Renfro'],\n",
       " ['Lori Tritsch', 'William Lauder'],\n",
       " ['Margaret Russell'],\n",
       " ['Martha Stewart', 'Laura Pla', 'Tomas Maier', 'Andrew Preston'],\n",
       " ['Margaret Russell', 'Carlos Souza'],\n",
       " ['Honoree Guillaume', 'Elizabeth Stribling', 'Stanfield Pinel'],\n",
       " ['Jean Shafiroff',\n",
       "  'Ursula Lowerre',\n",
       "  'Van Ness',\n",
       "  'Kazie Harvey',\n",
       "  'Deborah Royce'],\n",
       " ['Guy Robinson', 'Fernanda Kellogg', 'Kirk Henckels'],\n",
       " ['Bruce Horten'],\n",
       " ['Penny Grant', 'Michel Witmer'],\n",
       " ['Laurie Bodor', 'Ivey Long'],\n",
       " ['Cole Rumbough', 'Bettina Bennett'],\n",
       " ['Anthony Maltese'],\n",
       " ['Irene Aitken', 'Konrad Keesee'],\n",
       " ['Chase Hager'],\n",
       " ['Timothy Corrigan'],\n",
       " ['Timothy Corrigan'],\n",
       " ['Alex Donner', 'Jean Shafiroff', 'Geoffrey Bradfield'],\n",
       " ['Van Ness', 'Elizabeth Stribling', 'Patricia Shiah'],\n",
       " ['Barbara Regna', 'Couri Hay'],\n",
       " ['Roy Keane', 'Ann Rapp'],\n",
       " ['Silvina Leone', 'Ben Aguilar'],\n",
       " ['Caroline Brownstone', 'Charles Revson'],\n",
       " ['Margaret Russell'],\n",
       " ['Dennis Trafny', 'Kathleen Mahaffey'],\n",
       " ['Kate Coyne', 'Robert Liberman', 'Libby Monaco'],\n",
       " ['Jason Adelman'],\n",
       " ['Arriana Boardman', 'Fulton Miller'],\n",
       " ['Fisher Allen', 'Virginia Wettlaufer', 'Lily Maddock', 'Travis Acquavella'],\n",
       " ['Dennis Trafny', 'Robert Hart'],\n",
       " ['Ann Hohenhaus', 'Michael Heaner'],\n",
       " ['Crystal Bentley', 'Edgar Carranza'],\n",
       " ['News Anchor', 'Michaela Pereira'],\n",
       " ['Katie Ford', 'Rights Honoree', 'Molly Gochman'],\n",
       " ['Swizz Beatz', 'Rocco Bray'],\n",
       " ['Rights Honoree', 'Jared Bobrow', 'Executive Director', 'Sandy Santana'],\n",
       " ['Joseph Belluck', 'Strickland Squadron'],\n",
       " ['Schott Ledes', 'George Ledes', 'Jackie Norden'],\n",
       " ['Mark Kostabi'],\n",
       " ['Robert Simon'],\n",
       " ['Richard Ledes', 'Jean Shafiroff', 'Paola Mieli'],\n",
       " ['Kipton Cronkite'],\n",
       " ['Cole Rumough', 'Kick Kennedy'],\n",
       " ['Brian Maher', 'Ann Jensen', 'Eric Jensen'],\n",
       " ['Alex Donner'],\n",
       " ['Vanderbilt Costin', 'Gregory Speck']]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_and_join_names('2014-celebrating-the-treasures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeda863",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now that we have some sample captions, let's start parsing names out of those captions.  There are many ways of going about this, and we leave the details up to you.  Some issues to consider:\n",
    "\n",
    "  1. Some captions are not useful: they contain long narrative texts that explain the event.  Try to find some heuristic rules to separate captions that are a list of names from those that are not.  A few heuristics include:\n",
    "    - Look for sentences (which have verbs) and as opposed to lists of nouns. For example, [`nltk` does part of speech tagging](http://www.nltk.org/book/ch05.html) but it is a little slow. There may also be heuristics that accomplish the same thing.\n",
    "    - Similarly, spaCy's [entity recognition](https://spacy.io/docs/usage/entity-recognition) could be useful here, but like `nltk` using `spaCy` will add to processing time.\n",
    "    - Look for commonly repeated threads (e.g. you might end up picking up the photo credits or people such as \"a friend\").\n",
    "    - Long captions are often not lists of people.  The cutoff is subjective, but for grading purposes **we set that cutoff at 250 characters**.\n",
    "  1. Many of the captions contain extraneous whitespace or other formatting issues you may need to deal with.\n",
    "  1. You will want to separate the captions based on various forms of punctuation.  Try using `re.split`, which is more sophisticated than `string.split`. **Note**: The reference solution uses regex exclusively for name parsing.\n",
    "  1. You might find a person named \"ra Lebenthal\".  There is no one by this name.  Any idea what might cause that?\n",
    "  1. This site is pretty formal and likes to say things like \"Mayor Michael Bloomberg\" after his election but \"Michael Bloomberg\" before his election.  Can you find other ('optional') titles that are being used?  They should probably be filtered out because they ultimately refer to the same person: \"Michael Bloomberg.\"\n",
    "  1. There is a special case you might find where couples are written as e.g. \"John and Mary Smith\". You will need to write some extra logic to make sure this properly parses to two names: \"John Smith\" and \"Mary Smith\".\n",
    "  1. When parsing names from captions, it can help to look at your output frequently and address the problems that you see coming up, iterating until you have a list that looks reasonable. This is the approach used in the reference solution. Because we can only asymptotically approach perfect identification and entity matching, we have to stop somewhere.\n",
    "  1. Your eye is very good at doing this sort of parsing.  You will find it helpful to look at a caption and the names you parse of out it. Do this for a selection of captions to detect potential issues.\n",
    "  1. You want to keep the names in a caption together - that's how we can tell they're connected to each other! You should get one list of names for each caption.\n",
    "  \n",
    "**Questions worth considering:**\n",
    "  1. Who is Patrick McMullan and should he be included in the results? How would you address this?\n",
    "  2. What else could you do to improve the quality of the graph's information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will want to make a function that takes in a caption and returns a list of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac024760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library defs\n",
    "\n",
    "import requests\n",
    "import re\n",
    "#import spacy\n",
    "#from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fb03e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def titlremover(capshun):\n",
    "    # Define the titles to be removed\n",
    "    titles = ['Sir', 'Count', 'Countess', \n",
    "              'CEO', 'Lord', 'Dutchess', \n",
    "              'Consul', 'Counsel', 'Lady', \n",
    "              'General', 'Major', 'Senator', \n",
    "              'Mayor', 'Officer', 'Chief', \n",
    "              'President', 'Executive', 'Princess', \n",
    "              'Trustee', 'honoree', 'Curator', 'Honorees', 'Honoree']\n",
    "\n",
    "    # Split the caption into words\n",
    "    words = capshun.split()\n",
    "\n",
    "    # Create a new list to store the words without titles\n",
    "    filtered_words = []\n",
    "\n",
    "    # Iterate through the words and add them to the new list if they are not in titles\n",
    "    for word in words:\n",
    "        if word not in titles:\n",
    "            filtered_words.append(word)\n",
    "\n",
    "    # Join the filtered words back into a string with spaces\n",
    "    capshun_without_titles = ' '.join(filtered_words)\n",
    "    return capshun_without_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "724b8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for removing titles\n",
    "\n",
    "def titlremover(capshun):\n",
    "    \n",
    "\n",
    "#remove titles\n",
    "    titles = ['Sir','Count','Countess', \n",
    "              'CEO', 'Lord', 'Dutchess', \n",
    "              'Consul','Counsel', 'Lady', \n",
    "              'General', 'Major', 'Senator', \n",
    "              'Mayor', 'Officer', 'Chief', \n",
    "              'President', 'Executive', 'Princess', \n",
    "              'Trustee', 'honoree', 'Curator','Honorees','Honoree','Co-chairs','Baroness']\n",
    "\n",
    "    title_del = capshun.split(' ')\n",
    "    for t in titles:\n",
    "        try:\n",
    "            #trim white space to left of title\n",
    "            title_index = title_del.index(t)\n",
    "            del title_del[0:title_index + 1]\n",
    "            capshun = title_del\n",
    "            capshun = ' '.join(capshun)\n",
    "        #dummy line to say move on past no title found error\n",
    "        except ValueError as e:\n",
    "            ty=0\n",
    "    return capshun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eac5a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trimming white space\n",
    "\n",
    "def whitetrimmer(capshun):\n",
    "\n",
    "    #remove parentheses and quotes\n",
    "    capshun = re.sub(r'\\([^())]*\\)', '', capshun)\n",
    "    capshun = re.sub(r'\\\"[^\"]*\\\"', '', capshun)\n",
    "    \n",
    "    #trim new line characters\n",
    "    capshun = re.sub('\\n','',capshun)\n",
    "\n",
    "    #extra space deletion\n",
    "    capshun = re.sub(' +',' ',capshun)\n",
    "    capshun = capshun.rstrip(' ')\n",
    "    capshun = capshun.lstrip(' ')\n",
    "    \n",
    "    #remove apostrophes\n",
    "    capshun = re.sub('\\'s','',capshun)\n",
    "    \n",
    "    \n",
    "    return capshun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e31e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating by spouse\n",
    "\n",
    "def spouse_sep(capshun):\n",
    "    #split by comma\n",
    "    capshun = re.split(r\"\\,\\s| with \",capshun)\n",
    "    \n",
    "    #matrix to be filled in case of list containing a couple\n",
    "    flatlist=[]\n",
    "    \n",
    "    #capshun = re.split(' with ',str(capshun))\n",
    "    #sub incidental and at beginning of name\n",
    "    for i in range(len(capshun)): \n",
    "        capshun[i] = re.sub('^and ','',capshun[i])\n",
    "        #regex split by and\n",
    "        splitted = re.split(r\" and\\sand | and \",capshun[i])\n",
    "        #for cases left with one name (prob just last name)\n",
    "        if len(splitted) == 1:\n",
    "            nu_split = re.split(' ',str(splitted))\n",
    "            \n",
    "            #this is for one name [no last]\n",
    "            if len(nu_split) == 1:\n",
    "                return None\n",
    "            #this is for one full name\n",
    "            else:\n",
    "                capshun[i] = splitted\n",
    "        \n",
    "        else:\n",
    "            #checking if married or unmarried couple\n",
    "            if len(splitted[0].split(' ')) == 1:\n",
    "                #married couple, so split by space on second name\n",
    "                marsplit = splitted[1].split(' ')\n",
    "                \n",
    "                if len(marsplit) == 1:\n",
    "                    del splitted\n",
    "                \n",
    "                else:\n",
    "\n",
    "                    splitted[0] = splitted[0]+ ' ' + marsplit[1]\n",
    "\n",
    "                    capshun[i] = splitted\n",
    "\n",
    "            else:\n",
    "                capshun[i] = splitted\n",
    "        \n",
    "    capshun=[element for sublist in capshun for element in sublist]\n",
    "        \n",
    "    return capshun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dbcf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove mr/mrs/dr\n",
    "\n",
    "def prefixrem(capshun):\n",
    "    capshun = re.sub('[DM]rs?\\.?\\s','',capshun)\n",
    "    \n",
    "    return capshun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f43698ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit caption to 250 characters\n",
    "\n",
    "def charlimit(capshun):\n",
    "    #limit caption character count to 250\n",
    "    if len(capshun) > 200:\n",
    "        return None\n",
    "    #if its less than 200 characters return the caption\n",
    "    else:\n",
    "        return capshun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7301a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove articles such as 'a, an, the...'\n",
    "\n",
    "def artrem(capshun):\n",
    "    #if a capshun starts with any of the articles, skip it\n",
    "    #first split by space then check for articles in [0]\n",
    "    splitted = re.split(' ',capshun)\n",
    "    \n",
    "    if splitted[0] == 'A' or splitted[0] == 'The' or splitted[0] == 'An':\n",
    "        return None\n",
    "    else:\n",
    "        return capshun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4b453c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfriend(capshun):\n",
    "    capshun = re.sub('(\\san?d?)\\s?a?\\s?(friends?)','',capshun)\n",
    "    \n",
    "    return capshun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c634920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using spacy to filter non name captions\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "def spacy_fun(capshun):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    doc = nlp(capshun)\n",
    "    sent_mat = []\n",
    "    for token in doc:\n",
    "        sent_mat.append(token.pos_)\n",
    "\n",
    "    try:\n",
    "        if sent_mat[0] == 'ADJ':\n",
    "            return None\n",
    "\n",
    "        else:\n",
    "            name_check = re.split(' ',capshun)\n",
    "            if len(name_check) == 1:\n",
    "                del capshun\n",
    "            else:\n",
    "                x = re.search('^[a-z]',name_check[1])\n",
    "                y = x is None\n",
    "\n",
    "                if y == True:\n",
    "                    return capshun\n",
    "                else:\n",
    "                    return None\n",
    "    except IndexError as e:\n",
    "        #print('bad capshun')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79c65db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final functions\n",
    "def get_names(party_name):\n",
    "    final_names = []\n",
    "    #for i in range(len(party_list)):\n",
    "    capshun_list = get_captions(party_name)\n",
    "    for i in range(len(capshun_list)):\n",
    "        capshun = capshun_list[i]\n",
    "        capshun = whitetrimmer(capshun)\n",
    "        x = charlimit(capshun)\n",
    "        y = x is None\n",
    "        if y != True:\n",
    "            x = artrem(capshun)\n",
    "            y = x is None\n",
    "            if y != True:  \n",
    "                capshun = prefixrem(capshun)\n",
    "                capshun = titlremover(capshun)\n",
    "                capshun = unfriend(capshun)\n",
    "                x = spouse_sep(capshun)\n",
    "                y = x is None\n",
    "                if y != True:\n",
    "                    final_names.append(x)\n",
    "                else:\n",
    "                    ty=0\n",
    "            else:\n",
    "                ty=0\n",
    "        else:\n",
    "            ty=0\n",
    "    #make a flattened list of final names\n",
    "    namesonly = [element for sublist in final_names for element in sublist]\n",
    "    return final_names, namesonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9c05cfc7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Randy Takian',\n",
       " 'Kamie Lightburn',\n",
       " 'Christopher Spitzmiller',\n",
       " 'Christopher Spitzmiller',\n",
       " 'Diana Quasha',\n",
       " 'Mariam Azarm',\n",
       " 'Sana Sabbagh',\n",
       " 'Lynette Dallas',\n",
       " 'Christopher Spitzmiller',\n",
       " 'Sydney Shuman',\n",
       " 'Matthew Bees',\n",
       " 'Christopher Spitzmiller',\n",
       " 'Tom Edelman',\n",
       " 'Warren Scharf',\n",
       " 'Sydney Shuman',\n",
       " 'Amory McAndrew',\n",
       " 'Sean McAndrew',\n",
       " 'Sydney Shuman',\n",
       " 'Mario Buatta',\n",
       " 'Helene Tilney',\n",
       " 'Katherine DeConti',\n",
       " 'Elijah Duckworth-Schachter',\n",
       " 'John Rosselli',\n",
       " 'Elizabeth Swartz',\n",
       " 'Stephen Simcock',\n",
       " 'Lee Strock',\n",
       " 'Thomas Hammer',\n",
       " 'Searcy Dryden',\n",
       " 'Lesley Dryden',\n",
       " 'Richard Lightburn',\n",
       " 'Michel Witmer',\n",
       " 'Jennifer Cacioppo',\n",
       " 'Kevin Michael Barba',\n",
       " 'Virginia Wilbanks',\n",
       " 'Lacary Sharpe',\n",
       " 'Valentin Hernandez',\n",
       " 'Yaz Hernandez',\n",
       " 'Chele Farley',\n",
       " 'James Farley',\n",
       " 'Harry Heissmann',\n",
       " 'Angela Clofine',\n",
       " 'Michael Clofine',\n",
       " 'Jared Goss',\n",
       " 'Kristina Stewart Ward',\n",
       " 'Alex Papachristidis',\n",
       " 'Mario Buatta',\n",
       " 'Nick Olsen',\n",
       " 'Lindsey Coral Harper',\n",
       " 'Alberto Villalobos',\n",
       " 'David Duncan',\n",
       " 'Caroline Dean',\n",
       " 'Christopher Spitzmiller',\n",
       " 'Ellen Niven',\n",
       " 'Debbie Bancroft',\n",
       " 'David Svanda',\n",
       " 'Julia Weld',\n",
       " 'Christopher Spitzmiller',\n",
       " 'Kevin Lichten',\n",
       " 'Robert Ruffino',\n",
       " 'Joan Craig',\n",
       " 'Michael McGraw',\n",
       " 'Roric Tobin',\n",
       " 'Chele Farley',\n",
       " 'Richard Farley',\n",
       " 'Geoffrey Bradfield',\n",
       " 'Lacary Sharpe',\n",
       " 'Kamie Lightburn',\n",
       " 'Emily Leonard',\n",
       " 'Doug Steinbrech',\n",
       " 'Mary Van Pelt',\n",
       " 'Kathy Prounis',\n",
       " 'Othon Prounis',\n",
       " 'Victoire de Castellane',\n",
       " 'Dennis Freedman',\n",
       " 'Andi Potamkin',\n",
       " 'Ralph Pucci',\n",
       " 'Donald Tober',\n",
       " 'Barbara Tober',\n",
       " 'Linda Fargo',\n",
       " 'Bjorn Wallander',\n",
       " 'Andi Potamkin',\n",
       " 'Glenn Adamson',\n",
       " 'Ralph Pucci',\n",
       " 'Victoire de Castellane',\n",
       " 'Dennis Freedman',\n",
       " 'Craig Leavitt',\n",
       " 'Celia Morrissette',\n",
       " 'Keith Johnson',\n",
       " 'Simon Doonan',\n",
       " 'Sandra Gering',\n",
       " 'Julia Kuni',\n",
       " 'Alexandra Richards',\n",
       " 'Paul Longo',\n",
       " 'Bryna Pomp',\n",
       " 'Martin Pomp',\n",
       " 'Neville Wakefield',\n",
       " 'Caroline Cokley',\n",
       " 'Ryan Davis',\n",
       " 'Freddie Leiba',\n",
       " 'Cynthia Adler',\n",
       " 'Rick Presutti',\n",
       " 'Leticia Presutti',\n",
       " 'Glenn Adamson',\n",
       " 'Ebony G. Patterson',\n",
       " 'Lowery Stokes Sims',\n",
       " 'C. Virginia Fields',\n",
       " 'Barbara Regna',\n",
       " 'Deborah Lloyd',\n",
       " 'Kelly Sugarman',\n",
       " 'Jay Sugarman',\n",
       " 'Steve Evans',\n",
       " 'Kate White',\n",
       " 'Liz Nacey',\n",
       " 'Anne Strickland',\n",
       " 'Patrick Jones',\n",
       " 'Jennifer Fujitani',\n",
       " 'Wendy Diamond',\n",
       " 'Tood Eberle',\n",
       " 'Gina Nanni',\n",
       " 'Linda Fargo',\n",
       " 'Bjorn Wallander',\n",
       " 'Tony Ingrao',\n",
       " 'Randy Kemper',\n",
       " 'C. Virginia Fields',\n",
       " 'Gordan Kenney',\n",
       " 'Jill Huggins',\n",
       " 'Wendy MacGaw',\n",
       " 'Howard Ben Tre',\n",
       " 'Laura Kruger',\n",
       " 'Lewis Kruger',\n",
       " 'Deborah Marton',\n",
       " 'Bette Midler',\n",
       " 'Lance LePere',\n",
       " 'Michael Kors',\n",
       " 'Linda Allard party',\n",
       " 'Ben Needell Esq.',\n",
       " 'Nile Rodgers',\n",
       " 'Bette Midler',\n",
       " 'Todd DeGarmo',\n",
       " 'Deborah Marton',\n",
       " 'Samuel Kelly',\n",
       " 'David Rockwell',\n",
       " 'Lola Rockwell',\n",
       " 'Davon Windsor',\n",
       " 'Rachel Hilbert',\n",
       " 'Hula girls welcome the guests',\n",
       " 'Sheryl Tishman',\n",
       " 'Dan Tishman',\n",
       " 'Bette Midler',\n",
       " 'Darcy Stacom',\n",
       " 'Judy Gold',\n",
       " 'Bette Midler',\n",
       " 'Kevin Fox',\n",
       " 'Michelle Fox',\n",
       " 'Jane Krakowski',\n",
       " 'Michael Kors',\n",
       " 'Bette Midler',\n",
       " 'Lance LePere',\n",
       " 'Shoshanna Gruss',\n",
       " 'Drew Schiff',\n",
       " 'Margo Nederlander',\n",
       " 'Jimmy Nederlander',\n",
       " 'Nederlander table as Zombie Gillian’s Island',\n",
       " 'Jon Recor',\n",
       " 'Shelly Malkin',\n",
       " 'Nathalie Gerschel Kaplan',\n",
       " 'Zombie dancers in silver corridor',\n",
       " 'Douglas Little',\n",
       " 'Zombie airplane staff directing guests around the party',\n",
       " 'Martin von Haselberg',\n",
       " 'Allen Swerdlick',\n",
       " \"Ta'Rhonda Jones of Empire\",\n",
       " 'Lance LePere',\n",
       " 'Jane Krakowski',\n",
       " 'Michael Kors',\n",
       " 'Kelly Bensimon',\n",
       " 'John Demsey',\n",
       " 'Michael Kors',\n",
       " 'Lauren duPont',\n",
       " 'Nancy Hunt',\n",
       " 'Bette Midler',\n",
       " 'Nile Rodgers',\n",
       " 'Nile Rodgers & CHIC',\n",
       " 'Bette Midler on stage',\n",
       " 'Thriller zombie dancers',\n",
       " 'Richard Soloway',\n",
       " 'Donna Soloway',\n",
       " 'Barbara Regna',\n",
       " 'Peter Regna',\n",
       " 'Jonas Barcellos',\n",
       " 'Michelle Halpern',\n",
       " 'Mark Robinson',\n",
       " 'Lisa Halpern',\n",
       " 'Clark Munnell',\n",
       " 'Esra Munnell',\n",
       " 'Elizabeth Johnson',\n",
       " 'Wendy Carduner',\n",
       " 'George Perry',\n",
       " 'Lisa Wilkenson',\n",
       " 'Tom Wilkenson',\n",
       " 'Stacy Reilly',\n",
       " 'Eveyln Subramaniam',\n",
       " 'Sarah Page',\n",
       " 'Chips Page',\n",
       " 'Jim Killerlane',\n",
       " 'Allison Minton',\n",
       " 'Liana Makkos',\n",
       " 'Ken Fishel',\n",
       " 'Maria Fishel',\n",
       " 'Tina Wong',\n",
       " 'George Perry',\n",
       " 'Marisa Rose',\n",
       " 'Eveyln Subramaniam',\n",
       " 'Alexis Mersentes',\n",
       " 'Doris Liebman']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_captions(party_name):\n",
    "    return requests.get('https://party-captions.tditrain.com/captions?party=' + party_name).json()['captions']\n",
    "\n",
    "final_names, namesonly = get_names('2015-celebrating-the-neighborhood')\n",
    "#final_names\n",
    "len(namesonly)\n",
    "namesonly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83977c",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 2: sample_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feaa4cd",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Once you feel that your algorithm is working well, parse all of the captions we got from the `2015-celebrating-the-neighborhood` party and extract all the names mentioned.  Sort them alphabetically, by first name, and return the first hundred unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6182893f",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "sample_names = np.unique(namesonly)[:100]\n",
    "\n",
    "grader.score('graph__sample_names', sample_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d6a9f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now, test your tools on a few other parties.  You will probably find that other parties have new issues in their captions that trip up your caption parser.  But don't worry if the parser isn't perfect - just try to get the easy cases for now. You may need to come back and refine it more for the later questions, however."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39243634",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Parsing all the parties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffcacb4",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Once you are satisfied that your parser is working, we want to run it for all of our parties. First, get the captions for all of the parties in our party list. If you haven't implemented some caching of the captions, you probably want to do this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb936f",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# It may take several minutes to fetch all the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d84d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting captions for each party in party_list\n",
    "lister = []\n",
    "#returns just captions for all parties in party list\n",
    "for i in range(len(party_list)):\n",
    "        x = get_captions(party_list[i]['name'])\n",
    "        lister.append(x)\n",
    "        \n",
    "#flatten list of captions for each party\n",
    "flat_list = [item for sublist in lister for item in sublist]\n",
    "len(flat_list)\n",
    "\n",
    "#remove extraneous new line entries\n",
    "new_list = [x for x in flat_list if re.search(r'\\w', x)]\n",
    "len(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0d98722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "#with open('nysd-partycaptions.pkd', 'wb') as f:\n",
    "    #dill.dump(new_list, f)\n",
    "    \n",
    "with open('nysd-partycaptions.pkd', 'rb') as f:\n",
    "    new_list = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33cf0bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-gala-guests'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get a list of just party names\n",
    "party_list777 = ', '.join(d['name'] for d in party_list)\n",
    "party_list777 = re.split(\", \",party_list777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0b6aff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit Final functions to take in new_list\n",
    "final_names1 = []\n",
    "\n",
    "for i in range(len(party_list777)-1):\n",
    "    i=i+1\n",
    "    #print(party_list777[i])\n",
    "    final_names = extract_and_join_names(party_list777[i])\n",
    "    final_names1.append(final_names)\n",
    "\n",
    "namesonly_final = [element for sublist in final_names1 for element in sublist]\n",
    "\n",
    "#save final names\n",
    "import dill\n",
    "\n",
    "with open('final_names_en.pkd', 'wb') as f:\n",
    "    dill.dump(final_names1, f)\n",
    "\n",
    "#save names only\n",
    "import dill\n",
    "\n",
    "with open('namesonly_final_en.pkd', 'wb') as f:\n",
    "    dill.dump(namesonly_final, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3a968c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('final_names2.pkd', 'rb') as f:\n",
    "    final_names1 = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce7ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('namesonly_final.pkd', 'rb') as f:\n",
    "    namesonly_final = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e7ae59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "namesonly_final = [element for sublist in namesonly_final for element in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "45d246b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87166"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "#number of names\n",
    "len(set(namesonly_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d33b7040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100938"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_pattern = r'\\d|the|&|presents|[Aa]ward|for|Dinner|[Mm]ember|guest|cocktail|board|Sponsorship|group'\n",
    "filtered_list = [item for item in namesonly_final if not re.search(regex_pattern, item)]\n",
    "\n",
    "len(set(filtered_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85d5becd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99635"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of captions\n",
    "sum(len(inner) for inner in modified_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aee9b1",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "And parse the names in each caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee9c98",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# You should have a list of names for each caption\n",
    "# Depending on how you set up your parser, this may take quite a while"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a5f8c",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You should end up with over 100,000 captions and roughly 110,000 names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cadaace",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7d985",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "For the remaining analysis, we think of the problem in terms of a\n",
    "[network](http://en.wikipedia.org/wiki/Computer_network) or a\n",
    "[graph](https://en.wikipedia.org/wiki/Graph_%28discrete_mathematics%29).  Any time a pair of people appear in a caption together, that is considered a link.  What we have described is more appropriately called an (undirected)\n",
    "[multigraph](http://en.wikipedia.org/wiki/Multigraph) with no self-loops, but this has an obvious analog in terms of an undirected [weighted graph](http://en.wikipedia.org/wiki/Graph_%28mathematics%29#Weighted_graph).\n",
    "\n",
    "In the remainder of this miniproject, we will analyze the social graph of the New York social elite.  We recommend using python's [`networkx`](https://networkx.github.io/) library to build this social graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434ce7e0",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import itertools  # itertools.combinations may be useful\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c1eec",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You should find you have roughly 200,000 distinct pairs of people appearing in photos together - corresponding to how many (weighted) edges there are in our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97eb992",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 3: degree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a849e",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The simplest question to ask is \"who is the most popular\"?  The easiest way to answer this question is to look at how many connections everyone has.  Return the top 100 people and their degree.  Remember that if an edge of the graph has weight 2, it counts for 2 in the degree.\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution:\n",
    "    \n",
    "    count:  100.0\n",
    "    mean:   202.2\n",
    "    std:     94.1\n",
    "    min:    132.0\n",
    "    25%:    146.8\n",
    "    50%:    169.5\n",
    "    75%:    212.0\n",
    "    max:    713.0\n",
    "    \n",
    "Note that these checkpoints are guidelines, you may not match them exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "826c3226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#last clean up to remove single letters and other extras\n",
    "modified_data = [\n",
    "    [\n",
    "        [i for i in nested_nested if not (len(i) == 1 or re.search(r'\\d|the|&|presents|[Aa]ward|for|Dinner|[Mm]ember|guest|cocktail|board|Sponsorship|group', i))]\n",
    "        for nested_nested in nested\n",
    "        if any(not (len(i) == 1 or re.search(r'\\d|the|&|presents|[Aa]ward|for|Dinner|[Mm]ember|guest|cocktail|board|Sponsorship|group', i)) for i in nested_nested)\n",
    "    ]\n",
    "    for nested in final_names1\n",
    "    if any(any(not (len(i) == 1 or re.search(r'\\d|the|&|presents|[Aa]ward|for|Dinner|[Mm]ember|guest|cocktail|board|Sponsorship|group', i)) for i in nested_nested) for nested_nested in nested)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bc40264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jean Shafiroff: 397\n",
      "Mark Gilbertson: 356\n",
      "Gillian Miniter: 239\n",
      "Geoffrey Bradfield: 208\n",
      "Andrew Saffir: 205\n",
      "Mario Buatta: 202\n",
      "Somers Farkas: 189\n",
      "Yaz Hernandez: 188\n",
      "Alexandra Lebenthal: 186\n",
      "Kamie Lightburn: 183\n",
      "Alina Cho: 179\n",
      "Eleanora Kennedy: 175\n",
      "Lucia Hwong Gordon: 175\n",
      "Debbie Bancroft: 174\n",
      "Sharon Bush: 167\n",
      "Muffie Potter Aston: 154\n",
      "Bettina Zilkha: 150\n",
      "Patrick McMullan: 149\n",
      "Martha Stewart: 146\n",
      "Barbara Tober: 146\n",
      "Allison Aston: 146\n",
      "Amy Fine Collins: 142\n",
      "Jamee Gregory: 139\n",
      "Grace Meigher: 129\n",
      "Liliana Cavendish: 128\n",
      "Deborah Norville: 127\n",
      "Margo Langenberg: 125\n",
      "Fernanda Kellogg: 124\n",
      "Karen Klopp: 124\n",
      "Leonard Lauder: 124\n",
      "Dennis Basso: 123\n",
      "Audrey Gruss: 122\n",
      "Nicole Miller: 121\n",
      "Christopher Hyland: 121\n",
      "Lydia Fenet: 120\n",
      "Donna Karan: 119\n",
      "Jennifer Creel: 118\n",
      "Elizabeth Stribling: 117\n",
      "Kipton Cronkite: 117\n",
      "Evelyn Lauder: 115\n",
      "Liz Peek: 112\n",
      "Bonnie Comley: 112\n",
      "Russell Simmons: 112\n",
      "Annette Rickel: 111\n",
      "Anka Palitz: 109\n",
      "Karen LeFrak: 109\n",
      "Michael Bloomberg: 109\n",
      "Adelina Wong Ettelson: 109\n",
      "Fe Fendi: 108\n",
      "Felicia Taylor: 108\n",
      "Alec Baldwin: 107\n",
      "Michele Herbert: 107\n",
      "Diana Taylor: 107\n",
      "Barbara Regna: 104\n",
      "Susan Shin: 104\n",
      "Bette Midler: 104\n",
      "Fern Mallis: 103\n",
      "Dayssi Olarte de Kanavos: 102\n",
      "Paula Zahn: 102\n",
      "Martha Glass: 101\n",
      "Amy Hoadley: 101\n",
      "Rosanna Scotto: 100\n",
      "Michele Gerber Klein: 99\n",
      "Tory Burch: 99\n",
      "R. Couri Hay: 98\n",
      "Carol Mack: 98\n",
      "Hilary Geary Ross: 96\n",
      "Dawne Marie Grannum: 96\n",
      "Beth Rudin DeWoody: 96\n",
      "Richard Johnson: 95\n",
      "Coco Kopelman: 95\n",
      "Mary Davidson: 95\n",
      "Lisa Anastos: 95\n",
      "Pamela Fiori: 95\n",
      "Alexandra Lind Rose: 94\n",
      "Georgina Schaeffer: 94\n",
      "Amy McFarland: 94\n",
      "Mary Van Pelt: 93\n",
      "Evelyn Tompkins: 93\n",
      "Alexia Hamm Ryan: 92\n",
      "Wendy Carduner: 92\n",
      "Charlotte Moss: 92\n",
      "Cynthia Lufkin: 92\n",
      "Coralie Charriol Paul: 91\n",
      "Margaret Russell: 90\n",
      "CeCe Black: 90\n",
      "Jamie Niven: 90\n",
      "Daniel Benedict: 90\n",
      "Chuck Scarborough: 87\n",
      "Janna Bullock: 86\n",
      "Steven Stolman: 86\n",
      "Melissa Berkelhammer: 85\n",
      "Susan Magazine: 85\n",
      "Claudia Overstrom: 84\n",
      "Roric Tobin: 84\n",
      "Nathalie Kaplan: 84\n",
      "Peter Lyden: 84\n",
      "Stewart Lane: 83\n",
      "Marcia Mishaan: 83\n",
      "Frederick Anderson: 83\n"
     ]
    }
   ],
   "source": [
    "#flaten list of all captions\n",
    "\n",
    "finalnames_final11= [element for sublist in modified_data for element in sublist]\n",
    "\n",
    "#Making network graph\n",
    "\n",
    "import itertools  # itertools.combinations may be useful\n",
    "import networkx as nx\n",
    "\n",
    "# Create an empty graph\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Loop through each list in the list of lists\n",
    "for name_list in finalnames_final11:\n",
    "    # Update the graph with combinations of names within each list\n",
    "    combinations = itertools.combinations(name_list, 2)\n",
    "    for pair in combinations:\n",
    "        name1, name2 = pair\n",
    "        if graph.has_edge(name1, name2):\n",
    "            # If the edge already exists, increment the edge weight\n",
    "            graph[name1][name2]['weight'] += 1\n",
    "        else:\n",
    "            # If the edge does not exist, add it with weight 1\n",
    "            graph.add_edge(name1, name2, weight=1)\n",
    "\n",
    "sorted_degrees = sorted(graph.degree, key=lambda x: x[1], reverse=True)\n",
    "top_100_degrees = sorted_degrees[:100]\n",
    "\n",
    "for key, value in top_100_degrees:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b363147b",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.9100\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "import heapq  # Heaps are efficient structures for tracking the largest\n",
    "              # elements in a collection.  Use introspection to find the\n",
    "              # function you need.\n",
    "degree = [('Alec Baldwin', 144)] * 100\n",
    "\n",
    "grader.score('graph__degree', top_100_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df730faa",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 4: PageRank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1133d5",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "A similar way to determine popularity is to look at their\n",
    "[PageRank](http://en.wikipedia.org/wiki/PageRank).  PageRank is used for web ranking and was originally\n",
    "[patented](http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=6285999) by Google and is essentially the stationary distribution of a [Markov\n",
    "chain](http://en.wikipedia.org/wiki/Markov_chain) implied by the social graph. You can implement this yourself or use the version in `networkx`.\n",
    "\n",
    "Use 0.85 as the damping parameter so that there is a 15% chance of jumping to another vertex at random.\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution:\n",
    "\n",
    "\n",
    "    count:  100.000000\n",
    "    mean:     0.000193\n",
    "    std:      0.000080\n",
    "    min:      0.000130\n",
    "    25%:      0.000144\n",
    "    50%:      0.000169\n",
    "    75%:      0.000206\n",
    "    max:      0.000646\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "eb592fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 PageRank Scores:\n",
      "Jean Shafiroff: 0.0007\n",
      "Mark Gilbertson: 0.0005\n",
      "Gillian Miniter: 0.0005\n",
      "Geoffrey Bradfield: 0.0004\n",
      "Andrew Saffir: 0.0004\n",
      "Alexandra Lebenthal: 0.0003\n",
      "Yaz Hernandez: 0.0003\n",
      "Mario Buatta: 0.0003\n",
      "Somers Farkas: 0.0003\n",
      "Debbie Bancroft: 0.0003\n",
      "Sharon Bush: 0.0003\n",
      "Kamie Lightburn: 0.0003\n",
      "Eleanora Kennedy: 0.0003\n",
      "Alina Cho: 0.0003\n",
      "Barbara Tober: 0.0003\n",
      "Lucia Hwong Gordon: 0.0003\n",
      "Bonnie Comley: 0.0002\n",
      "Muffie Potter Aston: 0.0002\n",
      "Jamee Gregory: 0.0002\n",
      "Bettina Zilkha: 0.0002\n",
      "Martha Stewart: 0.0002\n",
      "Elizabeth Stribling: 0.0002\n",
      "Amy Fine Collins: 0.0002\n",
      "Patrick McMullan: 0.0002\n",
      "Allison Aston: 0.0002\n",
      "Fernanda Kellogg: 0.0002\n",
      "Christopher Hyland: 0.0002\n",
      "Daniel Benedict: 0.0002\n",
      "Liliana Cavendish: 0.0002\n",
      "Russell Simmons: 0.0002\n",
      "Margo Langenberg: 0.0002\n",
      "Grace Meigher: 0.0002\n",
      "Lydia Fenet: 0.0002\n",
      "Dawne Marie Grannum: 0.0002\n",
      "Stewart Lane: 0.0002\n",
      "Dennis Basso: 0.0002\n",
      "Donna Karan: 0.0002\n",
      "Deborah Norville: 0.0002\n",
      "Karen LeFrak: 0.0002\n",
      "Leonard Lauder: 0.0002\n",
      "Barbara Regna: 0.0002\n",
      "Karen Klopp: 0.0002\n",
      "Kipton Cronkite: 0.0002\n",
      "Nicole Miller: 0.0002\n",
      "Anka Palitz: 0.0002\n",
      "Evelyn Lauder: 0.0002\n",
      "Michele Herbert: 0.0002\n",
      "Diana Taylor: 0.0002\n",
      "Audrey Gruss: 0.0002\n",
      "Alec Baldwin: 0.0002\n",
      "Michael Bloomberg: 0.0002\n",
      "Roric Tobin: 0.0002\n",
      "Fe Fendi: 0.0002\n",
      "Liz Peek: 0.0002\n",
      "R. Couri Hay: 0.0002\n",
      "Felicia Taylor: 0.0002\n",
      "Georgina Schaeffer: 0.0002\n",
      "Richard Johnson: 0.0002\n",
      "Steven Stolman: 0.0002\n",
      "Annette Rickel: 0.0002\n",
      "Amy Hoadley: 0.0002\n",
      "Bette Midler: 0.0002\n",
      "Mary Van Pelt: 0.0002\n",
      "Michele Gerber Klein: 0.0002\n",
      "Susan Shin: 0.0002\n",
      "Jennifer Creel: 0.0002\n",
      "Coco Kopelman: 0.0002\n",
      "Fern Mallis: 0.0002\n",
      "Janna Bullock: 0.0002\n",
      "Chuck Scarborough: 0.0002\n",
      "Amy McFarland: 0.0002\n",
      "Rosanna Scotto: 0.0002\n",
      "Adelina Wong Ettelson: 0.0002\n",
      "Mary Davidson: 0.0002\n",
      "Dayssi Olarte de Kanavos: 0.0001\n",
      "Gregory Long: 0.0001\n",
      "Patricia Shiah: 0.0001\n",
      "Lisa Anastos: 0.0001\n",
      "Paula Zahn: 0.0001\n",
      "Agnes Gund: 0.0001\n",
      "Martha Glass: 0.0001\n",
      "Edward Callaghan: 0.0001\n",
      "John Demsey: 0.0001\n",
      "Tory Burch: 0.0001\n",
      "Susan Magazine: 0.0001\n",
      "CeCe Black: 0.0001\n",
      "Hilary Geary Ross: 0.0001\n",
      "Cynthia Lufkin: 0.0001\n",
      "Beth Rudin DeWoody: 0.0001\n",
      "family: 0.0001\n",
      "Sylvester Miniter: 0.0001\n",
      "Alexandra Lind Rose: 0.0001\n",
      "Alexia Hamm Ryan: 0.0001\n",
      "Pamela Fiori: 0.0001\n",
      "Cassandra Seidenfeld: 0.0001\n",
      "Bunny Williams: 0.0001\n",
      "Margo Catsimatidis: 0.0001\n",
      "Lizzie Tisch: 0.0001\n",
      "Karen Pearl: 0.0001\n",
      "Kristian Laliberte: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Calculate PageRank with damping parameter 0.85\n",
    "pagerank_scores = nx.pagerank(graph, alpha=0.85)\n",
    "\n",
    "# Get the top 100 scores using heapq\n",
    "top_100_scores = heapq.nlargest(100, pagerank_scores.items(), key=lambda item: item[1])\n",
    "\n",
    "# Print the top 100 scores for each name\n",
    "print(\"Top 100 PageRank Scores:\")\n",
    "for name, score in top_100_scores:\n",
    "    print(f\"{name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "84544580",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.9900\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "pagerank = [('Martha Stewart', 0.00019312108706213307)] * 100\n",
    "\n",
    "grader.score('graph__pagerank', top_100_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408b8b1",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 5: best_friends\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f3a91",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Another interesting question is who tend to co-occur with each other.  Give us the 100 edges with the highest weights.\n",
    "\n",
    "Google these people and see what their connection is.  Can we use this to detect instances of infidelity?\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution:\n",
    "\n",
    "    count:  100.0\n",
    "    mean:    27.5\n",
    "    std:     16.9\n",
    "    min:     14.0\n",
    "    25%:     17.0\n",
    "    50%:     21.5\n",
    "    75%:     31.3\n",
    "    max:    122.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "38028bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Edges with Highest Weights:\n",
      "('Gillian Miniter', 'Sylvester Miniter'): Weight - 119\n",
      "('Bonnie Comley', 'Stewart Lane'): Weight - 74\n",
      "('Jamee Gregory', 'Peter Gregory'): Weight - 74\n",
      "('Daniel Benedict', 'Andrew Saffir'): Weight - 66\n",
      "('Geoffrey Bradfield', 'Roric Tobin'): Weight - 64\n",
      "('Barbara Tober', 'Donald Tober'): Weight - 60\n",
      "('Somers Farkas', 'Jonathan Farkas'): Weight - 55\n",
      "('Jean Shafiroff', 'Martin Shafiroff'): Weight - 50\n",
      "('Campion Platt', 'Tatiana Platt'): Weight - 49\n",
      "('Alexandra Lebenthal', 'Jay Diamond'): Weight - 46\n",
      "('Yaz Hernandez', 'Valentin Hernandez'): Weight - 44\n",
      "('Eleanora Kennedy', 'Michael Kennedy'): Weight - 44\n",
      "('Chappy Morris', 'Melissa Morris'): Weight - 43\n",
      "('Guy Robinson', 'Elizabeth Stribling'): Weight - 41\n",
      "('Peter Regna', 'Barbara Regna'): Weight - 41\n",
      "('Deborah Norville', 'Karl Wellner'): Weight - 41\n",
      "('Grace Meigher', 'Chris Meigher'): Weight - 40\n",
      "('Jonathan Tisch', 'Lizzie Tisch'): Weight - 40\n",
      "('Margo Catsimatidis', 'John Catsimatidis'): Weight - 40\n",
      "('Sessa von Richthofen', 'Richard Johnson'): Weight - 38\n",
      "('Hilary Geary Ross', 'Wilbur Ross'): Weight - 35\n",
      "('David Koch', 'Julia Koch'): Weight - 33\n",
      "('Frederick Anderson', 'Douglas Hannant'): Weight - 32\n",
      "('Coco Kopelman', 'Arie Kopelman'): Weight - 31\n",
      "('Michael Cominotto', 'Dennis Basso'): Weight - 30\n",
      "('Fernanda Kellogg', 'Kirk Henckels'): Weight - 30\n",
      "('R. Couri Hay', 'Janna Bullock'): Weight - 29\n",
      "('Charles Cohen', 'Clo Cohen'): Weight - 28\n",
      "('Nina Griscom', 'Leonel Piraino'): Weight - 27\n",
      "('Dan Lufkin', 'Cynthia Lufkin'): Weight - 25\n",
      "('Melania Trump', 'Donald Trump'): Weight - 25\n",
      "('Tommy Hilfiger', 'Dee Ocleppo'): Weight - 24\n",
      "('Marc Rosen', 'Arlene Dahl'): Weight - 24\n",
      "('Olivia Palermo', 'Johannes Huebl'): Weight - 24\n",
      "('Mark Badgley', 'James Mischka'): Weight - 24\n",
      "('Jay McInerney', 'Anne Hearst McInerney'): Weight - 23\n",
      "('Michael Kovner', 'Jean Doyen de Montaillou'): Weight - 23\n",
      "('Jean Shafiroff', 'Patricia Shiah'): Weight - 22\n",
      "('Eleanora Kennedy', 'Anna Safir'): Weight - 22\n",
      "('Liz Peek', 'Jeff Peek'): Weight - 22\n",
      "('Othon Prounis', 'Kathy Prounis'): Weight - 22\n",
      "('John Demsey', 'Alina Cho'): Weight - 22\n",
      "('Chuck Scarborough', 'Ellen Scarborough'): Weight - 22\n",
      "('Donna Soloway', 'Richard Soloway'): Weight - 21\n",
      "('Muffie Potter Aston', 'Sherrell Aston'): Weight - 21\n",
      "('Susan Burke', 'Coleman Burke'): Weight - 21\n",
      "('Mary Davidson', 'Marvin Davidson'): Weight - 21\n",
      "('Wilbur Ross', 'Hilary Ross'): Weight - 21\n",
      "('Harry Slatkin', 'Laura Slatkin'): Weight - 21\n",
      "('Deborah Roberts', 'Al Roker'): Weight - 20\n",
      "('CeCe Black', 'Lee Black'): Weight - 20\n",
      "('Rick Friedberg', 'Francine LeFrak'): Weight - 20\n",
      "('Somers Farkas', 'Muffie Potter Aston'): Weight - 20\n",
      "('Hunt Slonem', 'Liliana Cavendish'): Weight - 20\n",
      "('Renee Steinberg', 'Richard Steinberg'): Weight - 20\n",
      "('Brian Stewart', 'Stephanie Krieger'): Weight - 20\n",
      "('David Lauren', 'Lauren Bush'): Weight - 20\n",
      "('Rod Gilbert', 'Judy Gilbert'): Weight - 19\n",
      "('Tatiana Perkin', 'Thorne Perkin'): Weight - 19\n",
      "('Keytt Lundqvist', 'Alex Lundqvist'): Weight - 19\n",
      "('Naeem Khan', 'Ranjana Khan'): Weight - 19\n",
      "('Jean Shafiroff', 'Sharon Bush'): Weight - 18\n",
      "('Melanie Wambold', 'John Wambold'): Weight - 18\n",
      "('Randy Kemper', 'Tony Ingrao'): Weight - 18\n",
      "('Gayle Sobel', 'Howard Sobel'): Weight - 18\n",
      "('Jill Zarin', 'Bobby Zarin'): Weight - 18\n",
      "('Kathy Hilton', 'Rick Hilton'): Weight - 18\n",
      "('Marcia Mishaan', 'Richard Mishaan'): Weight - 18\n",
      "('Sharon Sondes', 'Geoffrey Thomas'): Weight - 18\n",
      "('Gillian Miniter', 'Serena Miniter'): Weight - 17\n",
      "('Ann Rapp', 'Roy Kean'): Weight - 17\n",
      "('Elaine Langone', 'Ken Langone'): Weight - 17\n",
      "('Alexandra Lebenthal', 'Claudia Lebenthal'): Weight - 17\n",
      "('Lee Black', 'Cece Black'): Weight - 17\n",
      "('Susan Magazine', 'Nicholas Scoppetta'): Weight - 17\n",
      "('Gillian Hearst Simonds', 'Christian Simonds'): Weight - 17\n",
      "('Ruben Toledo', 'Isabel Toledo'): Weight - 17\n",
      "('Jill Kargman', 'Harry Kargman'): Weight - 16\n",
      "('Mary Snow', 'Ian Snow'): Weight - 16\n",
      "('Gillian Miniter', 'Alexandra Lebenthal'): Weight - 15\n",
      "('Charlotte Ronson', 'Ali Wise'): Weight - 15\n",
      "('Debbie Bancroft', 'Tiffany Dubin'): Weight - 15\n",
      "('Daniel Benedict', 'Johannes Huebl'): Weight - 15\n",
      "('Bunny Williams', 'John Rosselli'): Weight - 15\n",
      "('Larry Herbert', 'Michele Herbert'): Weight - 15\n",
      "('Paola Rosenshein', 'Arnie Rosenshein'): Weight - 15\n",
      "('Nicole Miller', 'Kim Taipale'): Weight - 15\n",
      "('Linette Semino', 'Matt Semino'): Weight - 15\n",
      "('Ramona Singer', 'Mario Singer'): Weight - 15\n",
      "('Will Cotton', 'Rose Dergan'): Weight - 15\n",
      "('Philip Gorrivan', 'Lisa Gorrivan'): Weight - 15\n",
      "('Marcy Warren', 'Michael Warren'): Weight - 15\n",
      "('Deborah Royce', 'Chuck Royce'): Weight - 14\n",
      "('Jean Shafiroff', 'Lucia Hwong Gordon'): Weight - 14\n",
      "('Edward Callaghan', 'John Wegorzewski'): Weight - 14\n",
      "('Alec Baldwin', 'Hilaria Baldwin'): Weight - 14\n",
      "('Larry Wohl', 'Leesa Rowland'): Weight - 14\n",
      "('Samantha Yanks', 'David Yanks'): Weight - 14\n",
      "('Peter Rockefeller', 'Allison Rockefeller'): Weight - 14\n",
      "('Shirin von Wulffen', 'Frederic Fekkai'): Weight - 14\n"
     ]
    }
   ],
   "source": [
    "# Get the top 100 edges with the highest weights\n",
    "top_100_edges = sorted(graph.edges(data=True), key=lambda edge: edge[2]['weight'], reverse=True)[:100]\n",
    "\n",
    "# Format the output as a list of ((name1, name2), weight) tuples\n",
    "output = [((name1, name2), weight['weight']) for name1, name2, weight in top_100_edges]\n",
    "\n",
    "# Print the top 100 edges with their weights\n",
    "print(\"Top 100 Edges with Highest Weights:\")\n",
    "for edge, weight in output:\n",
    "    print(f\"{edge}: Weight - {weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ec7987e1",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.9800\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "best_friends = [(('Michael Kennedy', 'Eleanora Kennedy'), 41)] * 100\n",
    "\n",
    "grader.score('graph__best_friends', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6fd99",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2022 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
