{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4b6c1c",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2915ce0f",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from static_grader import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f865a1",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Spark Miniproject\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc25b183",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Stack Overflow is a collaboratively edited question-and-answer site originally focused on programming topics. Because of the variety of features tracked, including a variety of feedback metrics, it allows for some open-ended analysis of user behavior on the site.\n",
    "\n",
    "Stack Exchange (the parent organization) provides an anonymized [data dump](https://archive.org/details/stackexchange), and we'll use Spark to perform data manipulation, analysis, and machine learning on this data set. As a side note, there's also an online data explorer which allows you to query the data interactively.\n",
    "\n",
    "*Consider*: Do we need to use Spark to work with this data set? What are our alternatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18065b",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1284fe",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "**All questions in this miniproject can be done locally in this notebook (i.e. on your Jupyter pod).**  \n",
    "\n",
    "You are free to try running on a cloud service, but note that we have no resources to pay for you to try out these services.  (New users often get a limited amount of free credit to try a service.)  Also, the grader library will not be available, so you would have to get your answers into this notebook to submit to the grader.   See the appropriate lecture notebooks for information on how to use cloud services if you want to try them out.\n",
    "\n",
    "Python example workflow when **not** running in a Jupyter notebook:\n",
    "\n",
    "1. Edit source code in your `main.py` file, classes in a separate `classes.py` (class definitions need to be written in a separate file and then included at runtime).\n",
    "1. If you are using a cloud service, in order to make your code more flexible, it's recommended to incorporate command-line arguments that specify the location of the input data and where output should be written.\n",
    "``` python\n",
    "# Command line arguments using sysv or argparse in Python\n",
    "if __name__ == '__main__':\n",
    "    main(ARGS.input_dir, ARGS.output_dir)\n",
    "```\n",
    "1. Run locally using the `spark-submit` program on a chunk using, eg., `$SPARK_HOME/bin/spark-submit --py-files src/classes.py src/main.py data/stats results/stats/`  Note that long jobs using `spark-submit` may not finish before your server gets automatically shut down (our server only checks for running Jupyter notebooks to avoid shutting down).  \n",
    "1. Run on Amazon Web Services (AWS) once your testing and development are done.  Note that you will also have to load all of the input data on an AWS bucket.  (Similar statements apply if you were to use Google Cloud Platform (GCP) or other services.)  \n",
    "\n",
    "General tips when using `spark-submit` or working on a cloud service:\n",
    "* Try `cat output_dir/* | sort -n -t , -k 1.2 -o sorted_output` to concatenate your output files, which will be in `part-xxxxx` format.\n",
    "* You can alternatively access an interactive PySpark shell on your Jupyter pod with this command: `$SPARK_HOME/bin/pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf3e24",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Accessing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49c77f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The data is available on S3 (`s3://dataincubator-course/spark-stack-data`). There are three sub-folders, `allUsers`, `allPosts`, and `allVotes` which contain Gzipped XML.  The `allPosts` sub-folder will contain data with the following format:\n",
    "\n",
    "``` html\n",
    "<row Body=\"&lt;p&gt;I always validate my web pages, and I recommend you do the same BUT many large company websites DO NOT and cannot validate because the importance of the website looking exactly the same on all systems requires rules to be broken. &lt;/p&gt;&#10;&#10;&lt;p&gt;In general, valid websites help your page look good even on odd configurations (like cell phones) so you should always at least try to make it validate.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2008-10-12T20:26:29.397\" Id=\"195995\" LastActivityDate=\"2008-10-12T20:26:29.397\" OwnerDisplayName=\"Eric Wendelin\" OwnerUserId=\"25066\" ParentId=\"195973\" PostTypeId=\"2\" Score=\"0\" />\n",
    "```\n",
    "\n",
    "Data from the much smaller `stats.stackexchange.com` website (called \"Cross Validated\") is available in the same format on S3 (`s3://dataincubator-course/spark-stats-data`). This smaller data set will be used below in most questions to avoid working with the full data set for every question.\n",
    "\n",
    "The full schema is available as a text file, which can be downloaded with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bf9a69",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 4.6 KiB/4.6 KiB (54.7 KiB/s) with 1 file(s) remaining\r",
      "download: s3://dataincubator-course/spark-stats-data/stack_exchange_schema.txt to ./stack_exchange_schema.txt\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://dataincubator-course/spark-stats-data/stack_exchange_schema.txt ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d886f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You can either get the data by running the appropriate S3 commands in the terminal, or by running this block for the smaller stats data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6bf1b0",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "!mkdir -p spark-stats-data\n",
    "!aws s3 sync --exclude '*' --include 'all*' s3://dataincubator-course/spark-stats-data/ ./spark-stats-data\n",
    "!aws s3 sync --exclude '*' --include 'posts*zip' s3://dataincubator-course/spark-stats-data/ ./spark-stats-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e24e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00009.xml.gz\n",
      "part-00007.xml.gz\n",
      "part-00008.xml.gz\n",
      "part-00006.xml.gz\n",
      "part-00003.xml.gz\n",
      "part-00004.xml.gz\n",
      "part-00010.xml.gz\n",
      "part-00005.xml.gz\n",
      "part-00002.xml.gz\n",
      "part-00000.xml.gz\n",
      "part-00001.xml.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List all files and directories in the \"allPosts\" folder\n",
    "all_posts_contents = os.listdir(\"./spark-stats-data/allPosts\")\n",
    "\n",
    "# Print the contents\n",
    "for item in all_posts_contents:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f4c36",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "And to get the much larger full data set (be warned, this can take 20 or more minutes, so you may want to run it in the terminal to avoid locking up the notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8603b56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n",
      "<parent>\r\n",
      "  <row AcceptedAnswerId=\"15\" AnswerCount=\"5\" Body=\"&lt;p&gt;How should I elicit prior distributions from experts when fitting a Bayesian model?&lt;/p&gt;&#10;\" CommentCount=\"1\" CreationDate=\"2010-07-19T19:12:12.510\" FavoriteCount=\"17\" Id=\"1\" LastActivityDate=\"2010-09-15T21:08:26.077\" OwnerUserId=\"8\" PostTypeId=\"1\" Score=\"26\" Tags=\"&lt;bayesian&gt;&lt;prior&gt;&lt;elicitation&gt;\" Title=\"Eliciting priors from experts\" ViewCount=\"1457\" />\r\n",
      "  \r\n",
      "  <row AcceptedAnswerId=\"59\" AnswerCount=\"7\" Body=\"&lt;p&gt;In many different statistical methods there is an &quot;assumption of normality&quot;.  What is &quot;normality&quot; and how do I know if there is normality?&lt;/p&gt;&#10;\" CommentCount=\"1\" CreationDate=\"2010-07-19T19:12:57.157\" FavoriteCount=\"9\" Id=\"2\" LastActivityDate=\"2012-11-12T09:21:54.993\" LastEditDate=\"2010-08-07T17:56:44.800\" LastEditorUserId=\"88\" OwnerUserId=\"24\" PostTypeId=\"1\" Score=\"25\" Tags=\"&lt;distributions&gt;&lt;normality&gt;\" Title=\"What is normality?\" ViewCount=\"9803\" />\r\n",
      "  \r\n",
      "  <row AcceptedAnswerId=\"5\" AnswerCount=\"19\" Body=\"&lt;p&gt;What are some valuable Statistical Analysis open source projects available right now?&lt;/p&gt;&#10;&#10;&lt;p&gt;Edit: as pointed out by Sharpie, valuable could mean helping you get things done faster or more cheaply.&lt;/p&gt;&#10;\" CommentCount=\"4\" CommunityOwnedDate=\"2010-07-19T19:13:28.577\" CreationDate=\"2010-07-19T19:13:28.577\" FavoriteCount=\"37\" Id=\"3\" LastActivityDate=\"2013-05-27T14:48:36.927\" LastEditDate=\"2011-02-12T05:50:03.667\" LastEditorUserId=\"183\" OwnerUserId=\"18\" PostTypeId=\"1\" Score=\"58\" Tags=\"&lt;software&gt;&lt;open-source&gt;\" Title=\"What are some valuable Statistical Analysis open source projects?\" ViewCount=\"4157\" />\r\n",
      "  \r\n",
      "  <row AcceptedAnswerId=\"135\" AnswerCount=\"5\" Body=\"&lt;p&gt;I have two groups of data.  Each with a different distribution of multiple variables.  I'm trying to determine if these two groups' distributions are different in a statistically significant way.  I have the data in both raw form and binned up in easier to deal with discrete categories with frequency counts in each.  &lt;/p&gt;&#10;&#10;&lt;p&gt;What tests/procedures/methods should I use to determine whether or not these two groups are significantly different and how do I do that in SAS or R (or Orange)?&lt;/p&gt;&#10;\" CommentCount=\"2\" CreationDate=\"2010-07-19T19:13:31.617\" FavoriteCount=\"3\" Id=\"4\" LastActivityDate=\"2010-09-08T03:00:19.690\" OwnerUserId=\"23\" PostTypeId=\"1\" Score=\"13\" Tags=\"&lt;distributions&gt;&lt;statistical-significance&gt;\" Title=\"Assessing the significance of differences in distributions\" ViewCount=\"7685\" />\r\n",
      "  \r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "#office hours; there are empty lines that need to be filtered, all of the values are strings(including the numbers)\n",
    "#process these things using RDDs, do not read the lines in as dataframes\n",
    "#some rows are broken (not xml), throw out lines \n",
    "#use a library that parses xml (lxml.etree.fromstring)\n",
    "#this is spark-stack-data but we want spark-stats-data for the questions\n",
    "! zcat spark-stats-data/allPosts/part-00000.xml.gz | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863e50f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n',\n",
       " '<parent>\\n',\n",
       " '  <row AcceptedAnswerId=\"7\" AnswerCount=\"13\" Body=\"&lt;p&gt;I want to use a track-bar to change a form\\'s opacity.&lt;/p&gt;&#10;&#10;&lt;p&gt;This is my code:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;decimal trans = trackBar1.Value / 5000;&#10;this.Opacity = trans;&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;When I try to build it, I get this error:&lt;/p&gt;&#10;&#10;&lt;blockquote&gt;&#10;  &lt;p&gt;Cannot implicitly convert type \\'decimal\\' to \\'double\\'.&lt;/p&gt;&#10;&lt;/blockquote&gt;&#10;&#10;&lt;p&gt;I tried making &lt;code&gt;trans&lt;/code&gt; a &lt;code&gt;double&lt;/code&gt;, but then the control doesn\\'t work. This code has worked fine for me in VB.NET in the past. &lt;/p&gt;&#10;\" CommentCount=\"1\" CommunityOwnedDate=\"2012-10-31T16:42:47.213\" CreationDate=\"2008-07-31T21:42:52.667\" FavoriteCount=\"27\" Id=\"4\" LastActivityDate=\"2014-12-20T17:18:47.807\" LastEditDate=\"2014-07-28T10:02:50.557\" LastEditorDisplayName=\"Rich B\" LastEditorUserId=\"451518\" OwnerUserId=\"8\" PostTypeId=\"1\" Score=\"322\" Tags=\"&lt;c#&gt;&lt;winforms&gt;&lt;type-conversion&gt;&lt;opacity&gt;\" Title=\"When setting a form\\'s opacity should I use a decimal or double?\" ViewCount=\"21888\" />\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#office hours using gzip to open gzip files\n",
    "import gzip\n",
    "with gzip.open('spark-stack-data/allPosts/part-00000.xml.gz','rt') as f:\n",
    "    lines = f.readlines(1000)\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09bb2fb4",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "#Large Dataset!!\n",
    "!mkdir -p spark-stack-data\n",
    "!aws s3 sync --exclude '*' --include 'all*' s3://dataincubator-course/spark-stack-data/ ./spark-stack-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0efcb68",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Data input and parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232d89f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Some rows are split across multiple lines; these can be discarded. Incorrectly formatted XML can also be ignored. It is enough to simply skip problematic rows, the loss of data will not significantly impact our results on these large data sets.\n",
    "\n",
    "**You will need to handle XML parsing yourself.  Our solution uses `lxml.etree` in Python, and we would recommend using this tool yourself to handle the XML parsing.**\n",
    "\n",
    "The goal should be to have a parsing function that can be applied to the input data to access any desired XML elements. You might find it convenient to represent the post, votes, users, etc. data using [`namedtuples`](https://docs.python.org/3/library/collections.html?highlight=namedtuple#collections.namedtuple)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35c468",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb230a9d",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "This miniproject is divided into two parts, called `spark_data` and `spark_ml`. The first part is doing data analysis in spark, on both a small data set and a large one. This consists of the first six questions in the notebook. The second part is using Spark ML to do machine learning, and is the last two questions. They are distinguished both by sections in the notebook and the question names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a815810b",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Spark data section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47cd6f6",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Question 1: Bad XML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa3616b",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "This first question is a simple question to test your parsing code. Create an RDD of Post objects where each Post is a valid row of XML from the small \"Cross Validated\" (stats.stackexchange.com) `allPosts` data set.\n",
    "\n",
    "We are going to take several shortcuts to speed up and simplify our computations.  First, your parsing function should only attempt to parse rows that start with `<row` as these denote actual data entries. This should be done in Spark as the data is being read in from disk, without any pre-Spark processing. \n",
    "\n",
    "Return the total number of XML rows that started with `<row` that were subsequently **rejected** during your XML processing.  Note that the text is Unicode, and contains non-ASCII characters.  You may need to re-encode to UTF-8 (depending on your XML parser).\n",
    "\n",
    "**Note that this cleaned data set will be used for questions 1-5.**  (For questions 6-8, you want to similarly remove improperly formatted XML from that data before proceeding further.)  \n",
    "\n",
    "*Question*: Can you figure out what filters you need to put in place to avoid throwing parsing errors entirely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb375719",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#parse all xml lines and return count of lines that dont parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd24ddd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import os\n",
    "import lxml.etree as et\n",
    "import gzip\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c3178e",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 0: Rejected Rows Count: 1\n",
      "File 1: Rejected Rows Count: 0\n",
      "File 2: Rejected Rows Count: 0\n",
      "File 3: Rejected Rows Count: 0\n",
      "File 4: Rejected Rows Count: 1\n",
      "File 5: Rejected Rows Count: 6\n",
      "File 6: Rejected Rows Count: 7\n",
      "File 7: Rejected Rows Count: 9\n",
      "File 8: Rejected Rows Count: 220\n",
      "File 9: Rejected Rows Count: 237\n",
      "File 10: Rejected Rows Count: 300\n",
      "Total Rows Count: 108741\n"
     ]
    }
   ],
   "source": [
    "# Spark context initialization\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "def process_xml_file(file_path):\n",
    "    # Read the XML file using SparkContext\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        xml_rdd = sc.parallelize(f.readlines())\n",
    "\n",
    "    # Count the total number of rows in the XML file\n",
    "    total_rows = xml_rdd.count()\n",
    "\n",
    "    # Filter rows that start with <row\n",
    "    filtered_xml_rdd = xml_rdd.filter(lambda row: row.strip().startswith(\"<row\"))\n",
    "\n",
    "    # Parse XML rows and extract required attributes\n",
    "    parsed_rdd = filtered_xml_rdd.map(parse_xml_row).filter(lambda post: post is not None)\n",
    "\n",
    "    # Count the number of rows that were rejected\n",
    "    rejected_rows = filtered_xml_rdd.count() - parsed_rdd.count()\n",
    "\n",
    "    return parsed_rdd, rejected_rows\n",
    "\n",
    "def parse_xml_row(xml_row):\n",
    "    try:\n",
    "        root = et.fromstring(xml_row.strip())\n",
    "    except et.XMLSyntaxError:\n",
    "        # If the XML cannot be parsed, return None to indicate a rejected row\n",
    "        return None\n",
    "\n",
    "    # Re-encode the XML row text to UTF-8\n",
    "    xml_row_utf8 = xml_row.encode('utf-8')\n",
    "\n",
    "    # Return the re-encoded XML row as a string\n",
    "    return xml_row_utf8\n",
    "\n",
    "# Directory path\n",
    "all_posts_folder_path = \"./spark-stats-data/allPosts\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "all_files = os.listdir(all_posts_folder_path)\n",
    "\n",
    "# Get a list of all XML files in the folder with full paths\n",
    "xml_files = [os.path.join(all_posts_folder_path, file) for file in all_files if file.endswith(\".xml.gz\")]\n",
    "\n",
    "# Create an RDD of valid XML rows from all the files\n",
    "parsed_rdds_and_rejected_counts = [process_xml_file(file) for file in xml_files]\n",
    "\n",
    "# Collect and output the count of rejected rows for each file\n",
    "rejected_rows_counts = [rejected_rows for _, rejected_rows in parsed_rdds_and_rejected_counts]\n",
    "\n",
    "# Output the count of rejected rows for each file\n",
    "for idx, rejected_rows in enumerate(rejected_rows_counts):\n",
    "    print(f\"File {idx}: Rejected Rows Count: {rejected_rows}\")\n",
    "\n",
    "# Combine all parsed RDDs into a single RDD\n",
    "parsed_rdd = sc.union([parsed_rdd for parsed_rdd, _ in parsed_rdds_and_rejected_counts])\n",
    "\n",
    "# Check the count of rows in the combined parsed RDD\n",
    "total_rows_count = parsed_rdd.count()\n",
    "print(f\"Total Rows Count: {total_rows_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad526a33",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rejected_rows_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94518147",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('spark_data__bad_xml', sum(rejected_rows_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088b6a8",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Question 2: Favorites and scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f00933f",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "We're interested in looking for useful patterns in the data.  If we look at the Post data again (the smaller set, `stats.stackexchange.com`), we see that many things about each post are recorded.  We're going to start by looking to see if there is a relationship between the number of times a post was favorited (the `FavoriteCount`) and the `Score`.  The score is the number of times the post was upvoted minus the number of times it was downvoted, so it is a measure of how much a post was liked.  We'd expect posts with a higher number of favorites to have better scores, since they're both measurements of how good the post is.\n",
    "\n",
    "Let's aggregate posts by the number of favorites, and find the average score for each number of favorites.  Do this for the lowest 50 numbers of favorites.\n",
    "\n",
    "**If any field in the Posts or Users is missing, such as the `FavoriteCount`, you should assume it is zero. _Make this assumption for all questions going forward._**\n",
    "\n",
    "_Note:_ Before submitting, take a look at the numbers.  Do they follow the trend you expect?\n",
    "\n",
    "**Checkpoints**\n",
    "\n",
    "- Total score across all posts: 299469\n",
    "- Mean of first 50 favorite counts (averaging the keys themselves): 24.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7286b0d7",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#access RDD to show only rows for favorite count and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b6d53b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "#sc = SparkContext(\"local\", \"XML Parsing App\")\n",
    "\n",
    "# Parse the XML data and extract FavoriteCount and Score\n",
    "def parse_xml_row(xml_str):\n",
    "    root = ET.fromstring(xml_str)\n",
    "    fav_count = root.get('FavoriteCount')\n",
    "    if fav_count is None:\n",
    "        fav_count = 0\n",
    "    else:\n",
    "        fav_count = int(fav_count)\n",
    "    score = root.get('Score')\n",
    "    if score is None:\n",
    "        score = 0\n",
    "    else:\n",
    "        score = int(score)\n",
    "    return fav_count, score\n",
    "\n",
    "# Convert RDD elements into tuples of (FavoriteCount, Score)\n",
    "parsed_rdd_tuples = parsed_rdd.map(parse_xml_row)\n",
    "\n",
    "# Use combineByKey to aggregate and calculate sum and count of scores by FavoriteCount\n",
    "favorite_count_score_data = parsed_rdd_tuples.combineByKey(\n",
    "    lambda score: (score, 1),                  # Create a tuple with (score, count) for the first occurrence\n",
    "    lambda x, score: (x[0] + score, x[1] + 1),  # Add the score and increment the count for subsequent occurrences\n",
    "    lambda x, y: (x[0] + y[0], x[1] + y[1])    # Merge the sums and counts from different partitions\n",
    ")\n",
    "\n",
    "# Calculate the average score for each FavoriteCount\n",
    "favorite_count_avg_score_data = favorite_count_score_data.mapValues(lambda v: v[0] / v[1])\n",
    "\n",
    "# Sort the RDD by FavoriteCount and take the 50 lowest ones\n",
    "sorted_favorite_counts = favorite_count_avg_score_data.sortBy(lambda x: x[0])\n",
    "lowest_50_favorite_counts = sorted_favorite_counts.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67a304f1",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "favorite_score = [(0, 2.3398827696988396)]*50\n",
    "\n",
    "grader.score('spark_data__favorite_score', lowest_50_favorite_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398856b9",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Question 3: Answer percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061487f",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "Investigate the correlation between a user's reputation and the kind of posts they make. For the 99 users with the highest reputation, single out posts which are either questions or answers and look at the percentage of these posts that are answers: *(answers / (answers + questions))*. \n",
    "\n",
    "Return a tuple of their **user ID** and this fraction.\n",
    "\n",
    "You should also return (-1, fraction) to represent the case where you average over all users (so you will return 100 entries total).\n",
    "\n",
    "Again, you only need to run this on the statistics overflow set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abc009",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba8777",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "* Total questions: 52,060\n",
    "* Total answers: 55,304\n",
    "* Top 99 users' average reputation: 11893.464646464647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4d9e347",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!aws s3 sync --exclude '*' --include 'users*zip' s3://dataincubator-course/spark-stats-data/ ./spark-stats-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ebedde8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000.xml.gz\n",
      "part-00001.xml.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List all files and directories in the \"allUsers\" folder\n",
    "all_posts_contents = os.listdir(\"./spark-stats-data/allUsers\")\n",
    "\n",
    "# Print the contents\n",
    "for item in all_posts_contents:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b64ac6ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n",
      "<parent>\r\n",
      "  <row AboutMe=\"&lt;p&gt;Hi, I'm not really a person.&lt;/p&gt;&#10;&#10;&lt;p&gt;I'm a background process that helps keep this site clean!&lt;/p&gt;&#10;&#10;&lt;p&gt;I do things like&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Randomly poke old unanswered questions every hour so they get some attention&lt;/li&gt;&#10;&lt;li&gt;Own community questions and answers so nobody gets unnecessary reputation from them&lt;/li&gt;&#10;&lt;li&gt;Own downvotes on spam/evil posts that get permanently deleted&lt;/li&gt;&#10;&lt;li&gt;Own suggested edits from anonymous users&lt;/li&gt;&#10;&lt;li&gt;&lt;a href=&quot;http://meta.stackexchange.com/a/92006&quot;&gt;Remove abandoned questions&lt;/a&gt;&lt;/li&gt;&#10;&lt;/ul&gt;&#10;\" AccountId=\"-1\" CreationDate=\"2010-07-19T06:55:26.860\" DisplayName=\"Community\" DownVotes=\"2330\" Id=\"-1\" LastAccessDate=\"2010-07-19T06:55:26.860\" Location=\"on the server farm\" Reputation=\"1\" UpVotes=\"5831\" Views=\"0\" WebsiteUrl=\"http://meta.stackexchange.com/\" />\r\n",
      "  \r\n",
      "  <row AboutMe=\"&lt;p&gt;Developer on the StackOverflow team.  Find me on&lt;/p&gt;&#10;&#10;&lt;p&gt;&lt;a href=&quot;http://www.twitter.com/SuperDalgas&quot; rel=&quot;nofollow&quot;&gt;Twitter&lt;/a&gt;&#10;&lt;br&gt;&lt;br&gt;&#10;&lt;a href=&quot;http://blog.stackoverflow.com/2009/05/welcome-stack-overflow-valued-associate-00003/&quot;&gt;Stack Overflow Valued Associate #00003&lt;/a&gt;&lt;/p&gt;&#10;\" AccountId=\"2\" Age=\"38\" CreationDate=\"2010-07-19T14:01:36.697\" DisplayName=\"Geoff Dalgas\" DownVotes=\"0\" Id=\"2\" LastAccessDate=\"2015-02-05T19:58:19.133\" Location=\"Corvallis, OR\" Reputation=\"101\" UpVotes=\"3\" Views=\"26\" WebsiteUrl=\"http://stackoverflow.com\" />\r\n",
      "  \r\n",
      "  <row AboutMe=\"&lt;p&gt;&lt;a href=&quot;http://blog.stackoverflow.com/2009/01/welcome-stack-overflow-valued-associate-00002/&quot;&gt;Developer on the Stack Overflow team&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;Was dubbed &lt;strong&gt;SALTY SAILOR&lt;/strong&gt; by Jeff Atwood, as filth and flarn would oft-times fly when dealing with a particularly nasty bug!&lt;/p&gt;&#10;&#10;&lt;ul&gt;&#10;&lt;li&gt;Twitter me: &lt;a href=&quot;http://twitter.com/jarrod_dixon&quot; rel=&quot;nofollow&quot;&gt;jarrod_dixon&lt;/a&gt;&lt;/li&gt;&#10;&lt;li&gt;Email me: jarrod.m.dixon@gmail.com&lt;/li&gt;&#10;&lt;/ul&gt;&#10;\" AccountId=\"3\" Age=\"36\" CreationDate=\"2010-07-19T15:34:50.507\" DisplayName=\"Jarrod Dixon\" DownVotes=\"0\" Id=\"3\" LastAccessDate=\"2015-02-05T19:58:19.283\" Location=\"New York, NY\" Reputation=\"101\" UpVotes=\"21\" Views=\"22\" WebsiteUrl=\"http://stackoverflow.com\" />\r\n",
      "  \r\n",
      "  <row AboutMe=\"&lt;p&gt;currently at a startup in SF&lt;/p&gt;&#10;&#10;&lt;p&gt;formerly a dev at Stack Exchange :)&lt;/p&gt;&#10;\" AccountId=\"1998\" Age=\"29\" CreationDate=\"2010-07-19T19:03:27.400\" DisplayName=\"Emmett\" DownVotes=\"0\" Id=\"4\" LastAccessDate=\"2014-01-02T09:31:02.107\" Location=\"San Francisco, CA\" ProfileImageUrl=\"http://i.stack.imgur.com/d1oHX.jpg\" Reputation=\"101\" UpVotes=\"0\" Views=\"11\" WebsiteUrl=\"http://minesweeperonline.com\" />\r\n",
      "  \r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "#check out one of the files\n",
    "! zcat spark-stats-data/allUsers/part-00000.xml.gz | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ba23e57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import os\n",
    "import lxml.etree as et\n",
    "import gzip\n",
    "import lxml\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5cc2001",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Spark context initialization\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "def process_xml_file(file_path):\n",
    "    # Read the XML file using SparkContext\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        xml_rdd = sc.parallelize(f.readlines())\n",
    "\n",
    "    # Count the total number of rows in the XML file\n",
    "    total_rows = xml_rdd.count()\n",
    "\n",
    "    # Filter rows that start with <row\n",
    "    filtered_xml_rdd = xml_rdd.filter(lambda row: row.strip().startswith(\"<row\"))\n",
    "\n",
    "    # Parse XML rows and extract required attributes\n",
    "    parsed_rdd = filtered_xml_rdd.map(parse_xml_row).filter(lambda post: post is not None)\n",
    "\n",
    "    # Count the number of rows that were rejected\n",
    "    rejected_rows = filtered_xml_rdd.count() - parsed_rdd.count()\n",
    "\n",
    "    return parsed_rdd, rejected_rows\n",
    "\n",
    "def parse_xml_row(xml_row):\n",
    "    try:\n",
    "        root = et.fromstring(xml_row.strip())\n",
    "    except et.XMLSyntaxError:\n",
    "        # If the XML cannot be parsed, return None to indicate a rejected row\n",
    "        return None\n",
    "\n",
    "    # Re-encode the XML row text to UTF-8\n",
    "    xml_row_utf8 = xml_row.encode('utf-8')\n",
    "\n",
    "    # Return the re-encoded XML row as a string\n",
    "    return xml_row_utf8\n",
    "\n",
    "# Directory path\n",
    "all_posts_folder_path = \"./spark-stats-data/allUsers\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "all_files = os.listdir(all_posts_folder_path)\n",
    "\n",
    "# Get a list of all XML files in the folder with full paths\n",
    "xml_files = [os.path.join(all_posts_folder_path, file) for file in all_files if file.endswith(\".xml.gz\")]\n",
    "\n",
    "# Create an RDD of valid XML rows from all the files\n",
    "parsed_rdds_and_rejected_counts = [process_xml_file(file) for file in xml_files]\n",
    "\n",
    "# Combine all parsed RDDs into a single RDD\n",
    "parsed_rdd_user = sc.union([parsed_rdd for parsed_rdd, _ in parsed_rdds_and_rejected_counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9769c2e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Parse the XML data and extract ID and Reputation\n",
    "def parse_xml_row(xml_str):\n",
    "    root = ET.fromstring(xml_str)\n",
    "    ID = root.get('Id')\n",
    "    if ID is None:\n",
    "        ID = 0\n",
    "    else:\n",
    "        ID = int(ID)\n",
    "    Reputation = root.get('Reputation')\n",
    "    if Reputation is None:\n",
    "        Reputation = 0\n",
    "    else:\n",
    "        Reputation = int(Reputation)\n",
    "    return ID, Reputation\n",
    "\n",
    "# Convert RDD elements into tuples of (ID, Reputation)..take the first entry from here for final output\n",
    "user_rdd_tuples = parsed_rdd_user.map(parse_xml_row)\n",
    "\n",
    "# Sort the RDD in descending order of Reputation and take the top 99 entries\n",
    "top_99_rdd = user_rdd_tuples.sortBy(lambda x: x[1], ascending=False).take(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f904245",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Now make another filtered RDD for the OwnerUserID and PostTypeId from the parsed RDD from the second question\n",
    "\n",
    "# Parse the XML data and extract OwnerUserID and PostTypeId\n",
    "def parse_xml_row(xml_str):\n",
    "    root = ET.fromstring(xml_str)\n",
    "    OwnerUserID = root.get('OwnerUserId')\n",
    "    if OwnerUserID is None:\n",
    "        OwnerUserID = 0\n",
    "    else:\n",
    "        OwnerUserID = int(OwnerUserID)\n",
    "    PostTypeId = root.get('PostTypeId')\n",
    "    if PostTypeId is None:\n",
    "        PostTypeId = 0\n",
    "    else:\n",
    "        PostTypeId = int(PostTypeId)\n",
    "    return OwnerUserID, PostTypeId\n",
    "\n",
    "# Convert RDD elements into tuples of (OwnerUserId, PostTypeId)\n",
    "post_rdd_tuples = parsed_rdd.map(parse_xml_row)\n",
    "\n",
    "# Extract IDs from top_99_rdd\n",
    "top_99_ids = [id for id, _ in top_99_rdd]\n",
    "\n",
    "# Filter post_rdd_tuples based on the IDs in top_99_ids\n",
    "filtered_post_rdd = post_rdd_tuples.filter(lambda x: x[0] in top_99_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of answers and questions for each OwnerUserId\n",
    "count_by_owner = filtered_post_rdd.map(lambda x: (x[0], (1 if x[1] == 2 else 0, 1 if x[1] == 1 else 0))) \\\n",
    "                                .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "# Calculate the ratio of answers to total (answers + questions) for each OwnerUserId\n",
    "ratio_by_owner = count_by_owner.mapValues(lambda x: x[0] / (x[0] + x[1]))\n",
    "\n",
    "# Calculate the total number of answers and questions for all users\n",
    "total_answers = count_by_owner.map(lambda x: x[1][0]).sum()\n",
    "total_questions = count_by_owner.map(lambda x: x[1][1]).sum()\n",
    "\n",
    "# Calculate the ratio of answers to total (answers + questions) across all users\n",
    "overall_ratio = total_answers / (total_answers + total_questions)\n",
    "\n",
    "# Add the additional entry with OwnerUserId = -1 and ratio = 'fraction'\n",
    "additional_entry = sc.parallelize([(-1, overall_ratio)])\n",
    "\n",
    "# Combine the results with the additional entry using union\n",
    "results_with_additional = ratio_by_owner.map(lambda x: (x[0], x[1])).union(additional_entry)\n",
    "\n",
    "# Collect the results as a list of tuples (OwnerUserId, ratio)\n",
    "results = results_with_additional.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76f255",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "answer_percentage = [(7071, 0.9107142857142857)] * 100\n",
    "\n",
    "grader.score('spark_data__answer_percentage', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eb5716",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Question 4: First question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c654f",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "We'd expect the first **question** a user asks to be indicative of their future behavior.  We'll dig more into that in the next problem, but for now let's see the relationship between reputation and how long it took each person to ask their first question.\n",
    "\n",
    "For each user that asked a question, find the difference between when their account was created (`CreationDate` for the User) and when they asked their first question (`CreationDate` for their first question).  Return this time difference in days (round down, so 2.7 days counts as 2 days) for the 100 users with the highest reputation, in the form\n",
    "\n",
    "`(UserId, Days)`\n",
    "\n",
    "**Checkpoints**\n",
    "- Users that asked a question: 23134\n",
    "- Average number of days (round each user's days, then average): 30.1074258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438fc5ab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load previously parsed RDDs for this question\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb6ba0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Parse the XML data and extract ID Reputation and CreationDate for USERS\n",
    "def parse_xml_row(xml_str):\n",
    "    root = ET.fromstring(xml_str)\n",
    "    ID = root.get('Id')\n",
    "    if ID is None:\n",
    "        ID = 0\n",
    "    else:\n",
    "        ID = int(ID)\n",
    "    \n",
    "    Reputation = root.get('Reputation')\n",
    "    if Reputation is None:\n",
    "        Reputation = 0\n",
    "    else:\n",
    "        Reputation = int(Reputation)\n",
    "        \n",
    "    CreationDate = root.get('CreationDate')\n",
    "    if CreationDate is None:\n",
    "        CreationDate = datetime.datetime.min\n",
    "    else:\n",
    "        try:\n",
    "            CreationDate = datetime.datetime.strptime(CreationDate, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            # If the format doesn't match, handle the error or set an appropriate default value\n",
    "            CreationDate = datetime.datetime.min\n",
    "    \n",
    "    return ID, CreationDate,Reputation\n",
    "\n",
    "# Convert RDD elements into tuples of (ID, CreationDate)..take the first entry from here for final output\n",
    "Q4user_rdd_tuples = parsed_rdd_user.map(parse_xml_row)\n",
    "\n",
    "# Convert RDD elements into DataFrames\n",
    "Q4user_df = Q4user_rdd_tuples.toDF([\"ID\", \"CreationDate\",\"Reputation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa22675",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Parse for Posts\n",
    "def parse_xml_row(xml_str):\n",
    "    root = ET.fromstring(xml_str)\n",
    "    OwnerUserId = root.get('OwnerUserId')\n",
    "    if OwnerUserId is None:\n",
    "        OwnerUserId = 0\n",
    "    else:\n",
    "        OwnerUserId = int(OwnerUserId)\n",
    "    \n",
    "    PostCreationDate = root.get('CreationDate')\n",
    "    if PostCreationDate is None:\n",
    "        PostCreationDate = datetime.datetime.min\n",
    "    else:\n",
    "        try:\n",
    "            PostCreationDate = datetime.datetime.strptime(PostCreationDate, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            PostCreationDate = datetime.datetime.min\n",
    "    \n",
    "    PostTypeId = root.get('PostTypeId')\n",
    "    if PostTypeId is None:\n",
    "        PostTypeId = 0\n",
    "    else:\n",
    "        PostTypeId = int(PostTypeId)\n",
    "        \n",
    "    return OwnerUserId, PostCreationDate, PostTypeId\n",
    "\n",
    "# Convert RDD elements into tuples of (OwnerUserId, CreationDate, PostTypeId)\n",
    "Q4post_rdd_tuples = parsed_rdd.map(parse_xml_row)\n",
    "\n",
    "# Filter out entries with PostTypeId as 1 (representing questions)\n",
    "Q4post_rdd_tuples_filtered = Q4post_rdd_tuples.filter(lambda x: x[2] == 1)\n",
    "\n",
    "# Convert RDD elements into DataFrame\n",
    "Q4post_df = Q4post_rdd_tuples_filtered.toDF([\"OwnerUserId\", \"CreationDate\", \"PostTypeId\"])\n",
    "\n",
    "# Group by OwnerUserId and return the earliest CreationDate for each user\n",
    "earliest_dates_df = Q4post_df.groupBy(\"OwnerUserId\").agg(F.min(\"CreationDate\").alias(\"EarliestPostDate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e77e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Join the DataFrames on ID and OwnerUserId\n",
    "joined_df = Q4user_df.join(earliest_dates_df, Q4user_df[\"ID\"] == earliest_dates_df[\"OwnerUserId\"])\n",
    "\n",
    "# Calculate the difference in days as a float\n",
    "joined_df = joined_df.withColumn(\"TimeDifference\", (F.col(\"EarliestPostDate\").cast(\"long\") - F.col(\"CreationDate\").cast(\"long\")) / 86400.0)\n",
    "\n",
    "# Round down the TimeDifference to the nearest integer\n",
    "joined_df = joined_df.withColumn(\"TimeDifference\", F.floor(F.col(\"TimeDifference\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bafbca7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Select the top 100 rows from the ordered DataFrame\n",
    "ordered_df = joined_df.orderBy(F.desc(\"Reputation\"))\n",
    "top_100_df = ordered_df.limit(100)\n",
    "\n",
    "# Extract the 'ID' and 'TimeDifference' columns from the selected rows\n",
    "result = top_100_df.select(\"ID\", \"TimeDifference\").collect()\n",
    "\n",
    "# Convert the result to a list of tuples\n",
    "list_of_tuples = [(row['ID'], row['TimeDifference']) for row in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be33154",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "first_question = [(805, 669)] * 100\n",
    "\n",
    "grader.score('spark_data__first_question', list_of_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8002f444",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Question 5: Identify veterans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5e040",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "It can be interesting to think about what factors influence a user to remain active on the site over a long period of time. In order not to bias the results towards older users, we'll define a time window between 100 and 150 days after account creation. If the user has made a post in this time, we'll consider them active and well on their way to being veterans of the site; if not, they are inactive and were likely brief users.\n",
    "\n",
    "*Consider*: What other parameterizations of \"activity\" could we use, and how would they differ in terms of splitting our user base?\n",
    "\n",
    "*Consider*: What other biases are still not dealt with, after using the above approach?\n",
    "\n",
    "Let's see if there are differences between the first ever question posts of \"veterans\" vs. \"brief users\". For each group separately, average the score, views, number of answers, and number of favorites of the users' **first question**. Remember, if the score, views, answers, or favorites is missing, you should assume it is zero.\n",
    "\n",
    "*Consider*: What story could you tell from these numbers? How do the numbers support it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3b769",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#identify users first question then use that to say whos vet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5bb47e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load previously parsed RDDs for this question\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9949b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b038ec6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Parse the XML data and extract ID Reputation and CreationDate for USERS\n",
    "def parse_xml_row(xml_str):\n",
    "    root = ET.fromstring(xml_str)\n",
    "    ID = root.get('Id')\n",
    "    if ID is None:\n",
    "        ID = 0\n",
    "    else:\n",
    "        ID = int(ID)\n",
    "    \n",
    "        UserCreationDate = root.get('CreationDate')\n",
    "    if UserCreationDate is None:\n",
    "        UserCreationDate = datetime.datetime.min\n",
    "    else:\n",
    "        try:\n",
    "            UserCreationDate = datetime.datetime.strptime(UserCreationDate, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            UserCreationDate = datetime.datetime.min\n",
    "    \n",
    "    return ID, UserCreationDate\n",
    "\n",
    "# Convert RDD elements into tuples of (ID, CreationDate)..take the first entry from here for final output\n",
    "Q5user_rdd_tuples = parsed_rdd_user.map(parse_xml_row)\n",
    "\n",
    "# Convert RDD elements into DataFrames\n",
    "Q5user_df = Q5user_rdd_tuples.toDF([\"ID\", \"UserCreationDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ddc1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Parse for Posts\n",
    "def parse_xml_row(xml_str):\n",
    "    root = ET.fromstring(xml_str)\n",
    "    OwnerUserId = root.get('OwnerUserId')\n",
    "    if OwnerUserId is None:\n",
    "        OwnerUserId = 0\n",
    "    else:\n",
    "        OwnerUserId = int(OwnerUserId)\n",
    "    \n",
    "    PostCreationDate = root.get('CreationDate')\n",
    "    if PostCreationDate is None:\n",
    "        PostCreationDate = datetime.datetime.min\n",
    "    else:\n",
    "        try:\n",
    "            PostCreationDate = datetime.datetime.strptime(PostCreationDate, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            PostCreationDate = datetime.datetime.min\n",
    "    \n",
    "    PostTypeId = root.get('PostTypeId')\n",
    "    if PostTypeId is None:\n",
    "        PostTypeId = 0\n",
    "    else:\n",
    "        PostTypeId = int(PostTypeId)\n",
    "        \n",
    "    Score = root.get('Score')\n",
    "    if Score is None:\n",
    "        Score = 0\n",
    "    else:\n",
    "        Score = int(Score)\n",
    "        \n",
    "    FavoriteCount = root.get('FavoriteCount')\n",
    "    if FavoriteCount is None:\n",
    "        FavoriteCount = 0 \n",
    "    else:\n",
    "        FavoriteCount = int(FavoriteCount)\n",
    "\n",
    "    AnswerCount = root.get('AnswerCount')\n",
    "    if AnswerCount is None:\n",
    "        AnswerCount = 0\n",
    "    else:\n",
    "        AnswerCount = int(AnswerCount)    \n",
    "    \n",
    "    Views = root.get('ViewCount')\n",
    "    if Views is None:\n",
    "        Views = 0\n",
    "    else:\n",
    "        Views = int(Views)\n",
    "    \n",
    "    return OwnerUserId, PostCreationDate, PostTypeId, Score, FavoriteCount,AnswerCount,Views\n",
    "\n",
    "# Convert RDD elements into tuples of (OwnerUserId, CreationDate, PostTypeId)\n",
    "Q5post_rdd_tuples = parsed_rdd.map(parse_xml_row)\n",
    "\n",
    "# Convert RDD elements into DataFrame\n",
    "Q5post_df = Q5post_rdd_tuples.toDF([\"OwnerUserId\", \"CreationDate\", \"PostTypeId\",\"Score\",\"FavoriteCount\",\"AnswerCount\",\"Views\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51c504",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Join the DataFrames on ID and OwnerUserId\n",
    "Q5joined_df = Q5post_df.join(Q5user_df, Q5post_df[\"OwnerUserId\"] == Q5user_df[\"ID\"])\n",
    "\n",
    "Q5joined_df = Q5joined_df.withColumn(\"TimeDifference\", (F.col(\"CreationDate\").cast(\"long\") - F.col(\"UserCreationDate\").cast(\"long\")) / 86400.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334ff06",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define the conditions to categorize users as \"Veteran\" or \"Brief\"\n",
    "veteran_condition = (Q5joined_df[\"TimeDifference\"] >= 100) & (Q5joined_df[\"TimeDifference\"] < 150)\n",
    "\n",
    "# Use the 'max' function over a window partitioned by 'OwnerUserId' to create a flag\n",
    "window = Window.partitionBy(\"OwnerUserId\")\n",
    "Q5joined_df = Q5joined_df.withColumn(\"IsVeteran\", F.max(F.when(veteran_condition, 1).otherwise(0)).over(window))\n",
    "\n",
    "# Use the 'when' function to apply the 'IsVeteran' flag and create the new column\n",
    "Q5joined_df = Q5joined_df.withColumn(\"UserType\", F.when(Q5joined_df[\"IsVeteran\"] == 1, \"Veteran\").otherwise(\"Brief\"))\n",
    "\n",
    "# Drop the temporary 'IsVeteran' column\n",
    "Q5joined_df = Q5joined_df.drop(\"IsVeteran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daaeb63",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Filter for questions only\n",
    "Q5joined_df = Q5joined_df.filter(Q5joined_df[\"PostTypeId\"] == 1)\n",
    "\n",
    "# Define a window specification to partition by OwnerUserId and order by CreationDate\n",
    "window_spec = Window.partitionBy(\"OwnerUserId\").orderBy(\"CreationDate\")\n",
    "\n",
    "# Add a new column \"EarliestCreationDate\" to store the earliest CreationDate for each OwnerUserId\n",
    "Q5joined_df = Q5joined_df.withColumn(\"EarliestCreationDate\", F.min(\"CreationDate\").over(window_spec))\n",
    "\n",
    "# Filter the DataFrame to keep only the rows with the earliest CreationDate\n",
    "active_users_df = Q5joined_df.filter(Q5joined_df[\"CreationDate\"] == Q5joined_df[\"EarliestCreationDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79382f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vet_df = active_users_df.filter(active_users_df[\"UserType\"] == \"Veteran\")\n",
    "brief_df = active_users_df.filter(active_users_df[\"UserType\"] == \"Brief\")\n",
    "\n",
    "# Calculate averages for veterans\n",
    "vet_averages = vet_df.agg(\n",
    "    F.avg(\"Score\").alias(\"vet_score\"),\n",
    "    F.avg(\"Views\").alias(\"vet_views\"),\n",
    "    F.avg(\"AnswerCount\").alias(\"vet_answers\"),\n",
    "    F.avg(\"FavoriteCount\").alias(\"vet_favorites\")\n",
    ")\n",
    "\n",
    "# Calculate averages for brief users\n",
    "brief_averages = brief_df.agg(\n",
    "    F.avg(\"Score\").alias(\"brief_score\"),\n",
    "    F.avg(\"Views\").alias(\"brief_views\"),\n",
    "    F.avg(\"AnswerCount\").alias(\"brief_answers\"),\n",
    "    F.avg(\"FavoriteCount\").alias(\"brief_favorites\")\n",
    ")\n",
    "\n",
    "# Collect the averages as a dictionary\n",
    "identify_veterans = vet_averages.crossJoin(brief_averages).collect()[0].asDict()\n",
    "\n",
    "# Show the resulting dictionary\n",
    "print(identify_veterans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c581de",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b928556",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "* Total brief users: 24,864\n",
    "* Total veteran users: 2,027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ba12e",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "grader.score('spark_data__identify_veterans', identify_veterans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd48944",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Question 6: Identify veterans&mdash;full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd648fc1",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "Same as above, but on the full Stack Exchange data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8844913",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#this will take about an hour to run, but should run easily after testing it in the question above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e8fc6ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "import os\n",
    "import lxml.etree as et\n",
    "import gzip\n",
    "import lxml\n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "482177dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#conf = pyspark.SparkConf()\n",
    "#conf.set(\"spark.executor.heartbeatInterval\",\"119s\")\n",
    "# make sure the executor doesn't die/timeout\n",
    "#sc = pyspark.SparkContext('local[*]', '', conf=conf)\n",
    "\n",
    "\n",
    "# Spark context initialization\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "#dont use parallizing, use textfile to read into spark since the data is massive\n",
    "def process_xml_file(file_path):\n",
    "    xml_rdd = sc.textFile(file_path)\n",
    "\n",
    "    # Filter rows that start with <row\n",
    "    filtered_xml_rdd = xml_rdd.filter(lambda row: row.strip().startswith(\"<row\"))\n",
    "\n",
    "    # Parse XML rows and extract required attributes\n",
    "    parsed_rdd = filtered_xml_rdd.map(parse_xml_row).filter(lambda post: post is not None)\n",
    "\n",
    "    # Count the number of rows that were rejected\n",
    "    rejected_rows = filtered_xml_rdd.count() - parsed_rdd.count()\n",
    "\n",
    "    return parsed_rdd, rejected_rows\n",
    "\n",
    "def parse_xml_row(xml_row):\n",
    "    try:\n",
    "        root = et.fromstring(xml_row.strip())\n",
    "    except et.XMLSyntaxError:\n",
    "        # If the XML cannot be parsed, return None to indicate a rejected row\n",
    "        return None\n",
    "\n",
    "    # Re-encode the XML row text to UTF-8\n",
    "    xml_row_utf8 = xml_row.encode('utf-8')\n",
    "\n",
    "    # Return the re-encoded XML row as a string\n",
    "    return xml_row_utf8\n",
    "\n",
    "# Directory path\n",
    "all_posts_folder_path = \"./spark-stack-data/allPosts\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "all_files = os.listdir(all_posts_folder_path)\n",
    "\n",
    "# Get a list of all XML files in the folder with full paths\n",
    "xml_files = [os.path.join(all_posts_folder_path, file) for file in all_files if file.endswith(\".xml.gz\")]\n",
    "\n",
    "# Create an RDD of valid XML rows from all the files\n",
    "parsed_rdds_and_rejected_counts = [process_xml_file(file) for file in xml_files]\n",
    "\n",
    "# Collect and output the count of rejected rows for each file\n",
    "rejected_rows_counts = [rejected_rows for _, rejected_rows in parsed_rdds_and_rejected_counts]\n",
    "\n",
    "# Combine all parsed RDDs into a single RDD\n",
    "parsed_rdd = sc.union([parsed_rdd for parsed_rdd, _ in parsed_rdds_and_rejected_counts])\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "def process_xml_file(file_path):\n",
    "    xml_rdd = sc.textFile(file_path)\n",
    "\n",
    "    # Filter rows that start with <row\n",
    "    filtered_xml_rdd = xml_rdd.filter(lambda row: row.strip().startswith(\"<row\"))\n",
    "\n",
    "    # Parse XML rows and extract required attributes\n",
    "    parsed_rdd = filtered_xml_rdd.map(parse_xml_row).filter(lambda post: post is not None)\n",
    "\n",
    "    # Count the number of rows that were rejected\n",
    "    rejected_rows = filtered_xml_rdd.count() - parsed_rdd.count()\n",
    "\n",
    "    return parsed_rdd, rejected_rows\n",
    "\n",
    "def parse_xml_row(xml_row):\n",
    "    try:\n",
    "        root = et.fromstring(xml_row.strip())\n",
    "    except et.XMLSyntaxError:\n",
    "        # If the XML cannot be parsed, return None to indicate a rejected row\n",
    "        return None\n",
    "\n",
    "    # Re-encode the XML row text to UTF-8\n",
    "    xml_row_utf8 = xml_row.encode('utf-8')\n",
    "\n",
    "    # Return the re-encoded XML row as a string\n",
    "    return xml_row_utf8\n",
    "\n",
    "# Directory path\n",
    "all_posts_folder_path = \"./spark-stack-data/allUsers\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "all_files = os.listdir(all_posts_folder_path)\n",
    "\n",
    "# Get a list of all XML files in the folder with full paths\n",
    "xml_files = [os.path.join(all_posts_folder_path, file) for file in all_files if file.endswith(\".xml.gz\")]\n",
    "\n",
    "# Create an RDD of valid XML rows from all the files\n",
    "parsed_rdds_and_rejected_counts = [process_xml_file(file) for file in xml_files]\n",
    "\n",
    "# Combine all parsed RDDs into a single RDD\n",
    "parsed_rdd_user = sc.union([parsed_rdd for parsed_rdd, _ in parsed_rdds_and_rejected_counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c039abfd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "######\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"7200s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Parse the XML data and extract ID Reputation and CreationDate for USERS\n",
    "def parse_xml_row(xml_str):\n",
    "    root = ET.fromstring(xml_str)\n",
    "    ID = root.get('Id')\n",
    "    if ID is None:\n",
    "        ID = 0\n",
    "    else:\n",
    "        ID = int(ID)\n",
    "    \n",
    "        UserCreationDate = root.get('CreationDate')\n",
    "    if UserCreationDate is None:\n",
    "        UserCreationDate = datetime.datetime.min\n",
    "    else:\n",
    "        try:\n",
    "            UserCreationDate = datetime.datetime.strptime(UserCreationDate, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            UserCreationDate = datetime.datetime.min\n",
    "    \n",
    "    return ID, UserCreationDate\n",
    "\n",
    "# Convert RDD elements into tuples of (ID, CreationDate)..take the first entry from here for final output\n",
    "Q5user_rdd_tuples = parsed_rdd_user.map(parse_xml_row)\n",
    "\n",
    "# Convert RDD elements into DataFrames\n",
    "Q5user_df = Q5user_rdd_tuples.toDF([\"ID\", \"UserCreationDate\"])\n",
    "\n",
    "#Parse for Posts\n",
    "def parse_xml_row(xml_str):\n",
    "    root = ET.fromstring(xml_str)\n",
    "    OwnerUserId = root.get('OwnerUserId')\n",
    "    if OwnerUserId is None:\n",
    "        OwnerUserId = 0\n",
    "    else:\n",
    "        OwnerUserId = int(OwnerUserId)\n",
    "    \n",
    "    PostCreationDate = root.get('CreationDate')\n",
    "    if PostCreationDate is None:\n",
    "        PostCreationDate = datetime.datetime.min\n",
    "    else:\n",
    "        try:\n",
    "            PostCreationDate = datetime.datetime.strptime(PostCreationDate, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            PostCreationDate = datetime.datetime.min\n",
    "    \n",
    "    PostTypeId = root.get('PostTypeId')\n",
    "    if PostTypeId is None:\n",
    "        PostTypeId = 0\n",
    "    else:\n",
    "        PostTypeId = int(PostTypeId)\n",
    "        \n",
    "    Score = root.get('Score')\n",
    "    if Score is None:\n",
    "        Score = 0\n",
    "    else:\n",
    "        Score = int(Score)\n",
    "        \n",
    "    FavoriteCount = root.get('FavoriteCount')\n",
    "    if FavoriteCount is None:\n",
    "        FavoriteCount = 0 \n",
    "    else:\n",
    "        FavoriteCount = int(FavoriteCount)\n",
    "\n",
    "    AnswerCount = root.get('AnswerCount')\n",
    "    if AnswerCount is None:\n",
    "        AnswerCount = 0\n",
    "    else:\n",
    "        AnswerCount = int(AnswerCount)    \n",
    "    \n",
    "    Views = root.get('ViewCount')\n",
    "    if Views is None:\n",
    "        Views = 0\n",
    "    else:\n",
    "        Views = int(Views)\n",
    "    \n",
    "    return OwnerUserId, PostCreationDate, PostTypeId, Score, FavoriteCount,AnswerCount,Views\n",
    "\n",
    "# Convert RDD elements into tuples of (OwnerUserId, CreationDate, PostTypeId)\n",
    "Q5post_rdd_tuples = parsed_rdd.map(parse_xml_row)\n",
    "\n",
    "# Convert RDD elements into DataFrame\n",
    "Q5post_df = Q5post_rdd_tuples.toDF([\"OwnerUserId\", \"CreationDate\", \"PostTypeId\",\"Score\",\"FavoriteCount\",\"AnswerCount\",\"Views\"])\n",
    "\n",
    "# Join the DataFrames on ID and OwnerUserId\n",
    "Q5joined_df = Q5post_df.join(Q5user_df, Q5post_df[\"OwnerUserId\"] == Q5user_df[\"ID\"])\n",
    "\n",
    "Q5joined_df = Q5joined_df.withColumn(\"TimeDifference\", (F.col(\"CreationDate\").cast(\"long\") - F.col(\"UserCreationDate\").cast(\"long\")) / 86400.0)\n",
    "\n",
    "# Define the conditions to categorize users as \"Veteran\" or \"Brief\"\n",
    "veteran_condition = (Q5joined_df[\"TimeDifference\"] >= 100) & (Q5joined_df[\"TimeDifference\"] < 150)\n",
    "\n",
    "# Use the 'max' function over a window partitioned by 'OwnerUserId' to create a flag\n",
    "window = Window.partitionBy(\"OwnerUserId\")\n",
    "Q5joined_df = Q5joined_df.withColumn(\"IsVeteran\", F.max(F.when(veteran_condition, 1).otherwise(0)).over(window))\n",
    "\n",
    "# Use the 'when' function to apply the 'IsVeteran' flag and create the new column\n",
    "Q5joined_df = Q5joined_df.withColumn(\"UserType\", F.when(Q5joined_df[\"IsVeteran\"] == 1, \"Veteran\").otherwise(\"Brief\"))\n",
    "\n",
    "# Drop the temporary 'IsVeteran' column\n",
    "Q5joined_df = Q5joined_df.drop(\"IsVeteran\")\n",
    "\n",
    "#Filter for questions only\n",
    "Q5joined_df = Q5joined_df.filter(Q5joined_df[\"PostTypeId\"] == 1)\n",
    "\n",
    "# Define a window specification to partition by OwnerUserId and order by CreationDate\n",
    "window_spec = Window.partitionBy(\"OwnerUserId\").orderBy(\"CreationDate\")\n",
    "\n",
    "# Add a new column \"EarliestCreationDate\" to store the earliest CreationDate for each OwnerUserId\n",
    "Q5joined_df = Q5joined_df.withColumn(\"EarliestCreationDate\", F.min(\"CreationDate\").over(window_spec))\n",
    "\n",
    "# Filter the DataFrame to keep only the rows with the earliest CreationDate\n",
    "active_users_df = Q5joined_df.filter(Q5joined_df[\"CreationDate\"] == Q5joined_df[\"EarliestCreationDate\"])\n",
    "\n",
    "vet_df = active_users_df.filter(active_users_df[\"UserType\"] == \"Veteran\")\n",
    "brief_df = active_users_df.filter(active_users_df[\"UserType\"] == \"Brief\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd95ac87",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vet_score': 2.2598437331442924, 'vet_views': 1844.0344896669696, 'vet_answers': 1.8426197044183144, 'vet_favorites': 0.8673157237744455, 'brief_score': 1.1307456144103445, 'brief_views': 1096.1519220732553, 'brief_answers': 1.5038565525030159, 'brief_favorites': 0.3861764445851408}\n"
     ]
    }
   ],
   "source": [
    "#sc = SparkContext.getOrCreate()\n",
    "# Create a SparkSession\n",
    "#spark = SparkSession.builder \\\n",
    "#    .config(\"spark.sql.broadcastTimeout\", \"7200s\") \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "# Calculate averages for veterans\n",
    "vet_averages = vet_df.agg(\n",
    "    F.avg(\"Score\").alias(\"vet_score\"),\n",
    "    F.avg(\"Views\").alias(\"vet_views\"),\n",
    "    F.avg(\"AnswerCount\").alias(\"vet_answers\"),\n",
    "    F.avg(\"FavoriteCount\").alias(\"vet_favorites\")\n",
    ")\n",
    "\n",
    "# Calculate averages for brief users\n",
    "brief_averages = brief_df.agg(\n",
    "    F.avg(\"Score\").alias(\"brief_score\"),\n",
    "    F.avg(\"Views\").alias(\"brief_views\"),\n",
    "    F.avg(\"AnswerCount\").alias(\"brief_answers\"),\n",
    "    F.avg(\"FavoriteCount\").alias(\"brief_favorites\")\n",
    ")\n",
    "\n",
    "# Collect the averages as a dictionary\n",
    "identify_veterans_full = vet_averages.crossJoin(brief_averages).collect()[0].asDict()\n",
    "\n",
    "# Show the resulting dictionary\n",
    "print(identify_veterans_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d10a3879",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28bb2b7",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ddbff2",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "* Total brief users: 1,848,628\n",
    "* Total veteran users: 288,285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "416d3a0e",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('spark_data__identify_veterans_full', identify_veterans_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb42d0",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "This ends the `spark_data` section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4cc88e",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Spark ML questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94513e",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "The questions from here forward are associated with the `spark_ml` prefix. They are working with Spark's ML library to do some NLP based analysis on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e753d7e",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": null
   },
   "source": [
    "## Question 7: Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64533bd0",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "Word2Vec is one approach for vectorizing text data. The vectorized representations of words in the vocabulary tend to be useful for predicting other words in the document, hence the famous example \"vector('king') - vector('man') + vector('woman') ~= vector('queen')\".\n",
    "\n",
    "Let's see how good a Word2Vec model we can train using the **tags** of each Stack Exchange post as documents (this uses the full data set). Use the implementation of Word2Vec from Spark ML (this will require using DataFrames) to return a list of the top 25 closest synonyms to \"ggplot2\" and their similarity score in tuple format (\"string\", number).\n",
    "\n",
    "The tags appear in the data as one string, you will need to separate them into individual tags. There is no need to further parse them beyond separating them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984aa39f",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "#### Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9fd1c",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "The dimensionality of the vector space should be 100. The random seed should be 42 in `PySpark`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9470a5",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d8ec8",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "source": [
    "* Mean of the top 25 cosine similarities: 0.8012362027168274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa28ab1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "import os\n",
    "import lxml.etree as et\n",
    "import gzip\n",
    "import lxml\n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffe93a83",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Word2VecExample\").getOrCreate()\n",
    "\n",
    "# Initialize Spark context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#dont use parallizing, use textfile to read into spark since the data is massive\n",
    "def process_xml_file(file_path):\n",
    "    xml_rdd = sc.textFile(file_path)\n",
    "\n",
    "    # Filter rows that start with <row\n",
    "    filtered_xml_rdd = xml_rdd.filter(lambda row: row.strip().startswith(\"<row\"))\n",
    "\n",
    "    # Parse XML rows and extract required attributes\n",
    "    parsed_rdd = filtered_xml_rdd.map(parse_xml_row).filter(lambda post: post is not None)\n",
    "\n",
    "    # Count the number of rows that were rejected\n",
    "    rejected_rows = filtered_xml_rdd.count() - parsed_rdd.count()\n",
    "\n",
    "    return parsed_rdd, rejected_rows\n",
    "\n",
    "def parse_xml_row(xml_row):\n",
    "    try:\n",
    "        root = et.fromstring(xml_row.strip())\n",
    "    except et.XMLSyntaxError:\n",
    "        # If the XML cannot be parsed, return None to indicate a rejected row\n",
    "        return None\n",
    "\n",
    "    # Re-encode the XML row text to UTF-8\n",
    "    xml_row_utf8 = xml_row.encode('utf-8')\n",
    "\n",
    "    # Return the re-encoded XML row as a string\n",
    "    return xml_row_utf8\n",
    "\n",
    "# Directory path\n",
    "all_posts_folder_path = \"./spark-stack-data/allPosts\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "all_files = os.listdir(all_posts_folder_path)\n",
    "\n",
    "# Get a list of all XML files in the folder with full paths\n",
    "xml_files = [os.path.join(all_posts_folder_path, file) for file in all_files if file.endswith(\".xml.gz\")]\n",
    "\n",
    "# Create an RDD of valid XML rows from all the files\n",
    "parsed_rdds_and_rejected_counts = [process_xml_file(file) for file in xml_files]\n",
    "\n",
    "# Collect and output the count of rejected rows for each file\n",
    "rejected_rows_counts = [rejected_rows for _, rejected_rows in parsed_rdds_and_rejected_counts]\n",
    "\n",
    "# Combine all parsed RDDs into a single RDD\n",
    "parsed_rdd = sc.union([parsed_rdd for parsed_rdd, _ in parsed_rdds_and_rejected_counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bff86768",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# Initialize Spark session\n",
    "#spark = SparkSession.builder.appName(\"XMLParsing\").getOrCreate()\n",
    "\n",
    "# Parse for Tags and OwnerUserId\n",
    "def parse_xml_row(xml_str):\n",
    "    root = ET.fromstring(xml_str)\n",
    "    owner_user_id = root.get('Id')\n",
    "    tags = root.get('Tags')\n",
    "    \n",
    "    if tags is None:\n",
    "        return []  # Return an empty list if Tags attribute is missing\n",
    "    else:\n",
    "        tag_list = [tag.strip('<>') for tag in tags.split('><')]\n",
    "        return [(owner_user_id, tag) for tag in tag_list]\n",
    "\n",
    "# Apply the parsing function to the RDD\n",
    "tags_owner_rdd = parsed_rdd.flatMap(parse_xml_row)\n",
    "\n",
    "# Define schema for DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Id\", StringType(), True),\n",
    "    StructField(\"Tag\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame from RDD with defined schema\n",
    "tags_owner_df = spark.createDataFrame(tags_owner_rdd, schema)\n",
    "\n",
    "# Group tags by OwnerUserId and collect them into an array\n",
    "grouped_tags_df = tags_owner_df.groupBy(\"Id\").agg(collect_list(\"Tag\").alias(\"Tags\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ed93330",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create Word2Vec model\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=10, inputCol=\"Tags\", outputCol=\"result\",seed=42)\n",
    "model = word2Vec.fit(grouped_tags_df)\n",
    "\n",
    "# Find synonyms for \"ggplot2\"\n",
    "synonyms = model.findSynonyms(\"ggplot2\", 25)\n",
    "\n",
    "# Extract synonyms and their similarity scores\n",
    "result = [(row.word, row.similarity) for row in synonyms.collect()]\n",
    "\n",
    "# Show the result\n",
    "#for word, similarity in result:\n",
    "#    print(f\"('{word}', {similarity})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "591cd968",
   "metadata": {
    "hidden": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "word2vec = [(\"data.frame\", 0.772650957107544)] * 25\n",
    "\n",
    "grader.score('spark_ml__word2vec', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb67c60",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 8: Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3f88d",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We'd like to see if we can predict the tags of a **question** from its body text. Instead of predicting specific tags, we will instead try to predict if a question contains one of the top ten most common tags.  \n",
    "\n",
    "To this end, we have separated out a train and a test set from the original data.  The training and tests sets were downloaded with the stats data at the beginning of the notebook.  You can also get them from S3:\n",
    "  * `s3://dataincubator-course/spark-stats-data/posts_train.zip`\n",
    "  * `s3://dataincubator-course/spark-stats-data/posts_test.zip`\n",
    "\n",
    "This will involve two steps: first, find the ten most common tags for questions in the training data set (the tags have been removed from the test set). Then train a learner to predict from the text of the question (the `Body` attribute) if it should have one of those ten tags in it - you will need to process the question text with NLP techniques such as splitting the text into tokens.\n",
    "\n",
    "Since we can't reliably pickle Spark models, instead return a list of your predictions, sorted by the question's `Id`.  This sorting is very important, as our grader expects the results to be submitted in a particular order. These predictions should be `0` if the question isn't expected to have a tag in the top ten, and `1` if it is.\n",
    "\n",
    "As an example, if our top tags include `spark` and `python`, and we had the following questions:\n",
    "\n",
    "```\n",
    "<row Body=\"...\" Id=\"1740\" Tags=\"<machine-learning><spark><regression>\" ... />\n",
    "<row Body=\"...\" Id=\"723\" Tags=\"<statistics><neurons>\" ... />\n",
    "<row Body=\"...\" Id=\"2740\" Tags=\"<functional><python><spark><pyspark>\" ... />\n",
    "```\n",
    "\n",
    "We would expect to return `[0, 1, 1]` (for the order `[723, 1740, 2740]`).  You may need to do some format manipulation in your DataFrame to get this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b4be9",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac32ab",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "- Number of training posts with a tag in the top 10: `22525`\n",
    "- Number without: `19540`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3dbce7ac",
   "metadata": {
    "collapsed": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  spark-stats-data/posts_train.zip\n",
      "  inflating: spark-stats-data/train/part-00001  \n",
      "  inflating: spark-stats-data/train/part-00002  \n",
      "  inflating: spark-stats-data/train/part-00003  \n",
      "  inflating: spark-stats-data/train/part-00004  \n",
      "  inflating: spark-stats-data/train/part-00005  \n",
      "  inflating: spark-stats-data/train/part-00006  \n",
      "  inflating: spark-stats-data/train/part-00007  \n",
      "  inflating: spark-stats-data/train/part-00008  \n",
      "  inflating: spark-stats-data/train/part-00009  \n",
      "  inflating: spark-stats-data/train/part-00010  \n",
      "Archive:  spark-stats-data/posts_test.zip\n",
      "  inflating: spark-stats-data/test/part-00001  \n",
      "  inflating: spark-stats-data/test/part-00002  \n",
      "  inflating: spark-stats-data/test/part-00003  \n",
      "  inflating: spark-stats-data/test/part-00004  \n",
      "  inflating: spark-stats-data/test/part-00005  \n",
      "  inflating: spark-stats-data/test/part-00006  \n",
      "  inflating: spark-stats-data/test/part-00007  \n",
      "  inflating: spark-stats-data/test/part-00008  \n",
      "  inflating: spark-stats-data/test/part-00009  \n",
      "  inflating: spark-stats-data/test/part-00010  \n"
     ]
    }
   ],
   "source": [
    "!unzip -u -d spark-stats-data/train spark-stats-data/posts_train.zip\n",
    "!unzip -u -d spark-stats-data/test spark-stats-data/posts_test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "025dd02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree as et\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF,Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"TagPrediction\").config(\"spark.seed\", 42).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "232ae4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#dont use parallizing, use textfile to read into spark since the data is massive\n",
    "def process_xml_file(file_path):\n",
    "    xml_rdd = sc.textFile(file_path)\n",
    "\n",
    "    # Filter rows that start with <row\n",
    "    filtered_xml_rdd = xml_rdd.filter(lambda row: row.strip().startswith(\"<row\"))\n",
    "\n",
    "    # Parse XML rows and extract required attributes\n",
    "    parsed_rdd = filtered_xml_rdd.map(parse_xml_row).filter(lambda post: post is not None)\n",
    "\n",
    "    # Count the number of rows that were rejected\n",
    "    rejected_rows = filtered_xml_rdd.count() - parsed_rdd.count()\n",
    "\n",
    "    return parsed_rdd, rejected_rows\n",
    "\n",
    "def parse_xml_row(xml_row):\n",
    "    try:\n",
    "        root = et.fromstring(xml_row.strip())\n",
    "    except et.XMLSyntaxError:\n",
    "        # If the XML cannot be parsed, return None to indicate a rejected row\n",
    "        return None\n",
    "\n",
    "    # Re-encode the XML row text to UTF-8\n",
    "    xml_row_utf8 = xml_row.encode('utf-8')\n",
    "\n",
    "    # Return the re-encoded XML row as a string\n",
    "    return xml_row_utf8\n",
    "\n",
    "# Directory path (test data)\n",
    "all_posts_folder_path = \"spark-stats-data/test\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "all_files = os.listdir(all_posts_folder_path)\n",
    "\n",
    "# Get a list of all XML files in the folder with full paths\n",
    "xml_files = [os.path.join(all_posts_folder_path, file) for file in all_files]\n",
    "\n",
    "# Create an RDD of valid XML rows from all the files\n",
    "parsed_rdds_and_rejected_counts = [process_xml_file(file) for file in xml_files]\n",
    "\n",
    "# Combine all parsed RDDs into a single RDD\n",
    "parsed_rdd_test = sc.union([parsed_rdd for parsed_rdd, _ in parsed_rdds_and_rejected_counts])\n",
    "\n",
    "# Convert byte-string to string and filter rows with PostTypeId=\"1\"\n",
    "parsed_rdd_test = parsed_rdd_test.filter(lambda xml_str: b'PostTypeId=\"1\"' in xml_str).map(lambda xml_bytes: xml_bytes.decode('utf-8'))\n",
    "\n",
    "# Directory path (Training data)\n",
    "all_posts_folder_path_train = \"spark-stats-data/train\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "all_files = os.listdir(all_posts_folder_path_train)\n",
    "\n",
    "# Get a list of all XML files in the folder with full paths\n",
    "xml_files = [os.path.join(all_posts_folder_path_train, file) for file in all_files]\n",
    "\n",
    "# Create an RDD of valid XML rows from all the files\n",
    "parsed_rdds_and_rejected_counts = [process_xml_file(file) for file in xml_files]\n",
    "\n",
    "# Combine all parsed RDDs into a single RDD\n",
    "parsed_rdd_train = sc.union([parsed_rdd_train for parsed_rdd_train, _ in parsed_rdds_and_rejected_counts])\n",
    "\n",
    "# Convert byte-string to string and filter rows with PostTypeId=\"1\"\n",
    "parsed_rdd_train = parsed_rdd_train.filter(lambda xml_str: b'PostTypeId=\"1\"' in xml_str).map(lambda xml_bytes: xml_bytes.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1905b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.sql.functions import explode, col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b65925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Spark session\n",
    "#spark = SparkSession.builder.appName(\"XMLParsing\").getOrCreate()\n",
    "\n",
    "# Parse for Body, Id, PostTypeId, and Tags\n",
    "def parse_xml_row(xml_str):\n",
    "    root = ET.fromstring(xml_str)\n",
    "    body = root.get('Body')\n",
    "    owner_user_id = root.get('Id')\n",
    "    post_type_id = root.get('PostTypeId')\n",
    "    tags = root.get('Tags')\n",
    "    \n",
    "    if tags is None:\n",
    "        return []  # Return an empty list if Tags attribute is missing\n",
    "    else:\n",
    "        tag_list = [tag.strip('<>') for tag in tags.split('><')]\n",
    "        return [(body, owner_user_id, post_type_id, tag) for tag in tag_list]\n",
    "\n",
    "# Apply the parsing function to the RDD\n",
    "tags_rdd_train = parsed_rdd_train.flatMap(parse_xml_row)\n",
    "\n",
    "# Define schema for DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Body\", StringType(), True),\n",
    "    StructField(\"Id\", StringType(), True),\n",
    "    StructField(\"PostTypeId\", StringType(), True),\n",
    "    StructField(\"Tags\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame from RDD with defined schema\n",
    "train_tags_df = spark.createDataFrame(tags_rdd_train, schema)\n",
    "\n",
    "# Apply the parsing function to the RDD\n",
    "tags_test_rdd = parsed_rdd_test.flatMap(parse_xml_row)\n",
    "\n",
    "# Create DataFrame from RDD with defined schema\n",
    "tags_test_df = spark.createDataFrame(tags_test_rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35a705d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group tags by OwnerUserId and collect them into an array\n",
    "train_tags_df_10 = train_tags_df.groupBy(\"Id\",\"Body\").agg(collect_list(\"Tags\").alias(\"Tags\"))\n",
    "\n",
    "# Flatten the DataFrame based on the 'Tags' column\n",
    "flattened_df = train_tags_df_10.select(\"Id\", \"Body\", explode(col(\"Tags\")).alias(\"Tag\"))\n",
    "\n",
    "# Group by 'Tags' and count the occurrences\n",
    "tag_counts = flattened_df.groupBy(\"Tag\").count()\n",
    "\n",
    "# Sort the results in descending order based on tag counts\n",
    "sorted_tag_counts = tag_counts.orderBy(col(\"count\").desc())\n",
    "\n",
    "# Select the top 10 occurrences\n",
    "top_10_tag_counts = sorted_tag_counts.limit(10)\n",
    "\n",
    "# Show the top 10 tag occurrences\n",
    "#top_10_tag_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11987a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, array_contains\n",
    "\n",
    "# Extract the top 10 tags from the DataFrame\n",
    "top_10_tags = top_10_tag_counts.select(\"Tag\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Convert the list of top 10 tags to a set for efficient lookup\n",
    "top_10_tags_set = set(top_10_tags)\n",
    "\n",
    "# Create a new column 'label' indicating whether any tag matches a top 10 tag\n",
    "train_tags_df_with_label = train_tags_df_10.withColumn(\"label\", \n",
    "                                                       when(array_contains(col(\"Tags\"), top_10_tags[0]) |\n",
    "                                                            array_contains(col(\"Tags\"), top_10_tags[1]) |\n",
    "                                                            array_contains(col(\"Tags\"), top_10_tags[2]) |\n",
    "                                                            array_contains(col(\"Tags\"), top_10_tags[3]) |\n",
    "                                                            array_contains(col(\"Tags\"), top_10_tags[4]) |\n",
    "                                                            array_contains(col(\"Tags\"), top_10_tags[5]) |\n",
    "                                                            array_contains(col(\"Tags\"), top_10_tags[6]) |\n",
    "                                                            array_contains(col(\"Tags\"), top_10_tags[7]) |\n",
    "                                                            array_contains(col(\"Tags\"), top_10_tags[8]) |\n",
    "                                                            array_contains(col(\"Tags\"), top_10_tags[9]),\n",
    "                                                            1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9329dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast 'label' column to integer\n",
    "train_tags_df_with_label = train_tags_df_with_label.withColumn(\"label\", col(\"label\").cast(\"int\"))\n",
    "\n",
    "# Cast 'Id' column to integer\n",
    "train_tags_df_with_label = train_tags_df_with_label.withColumn(\"Id\", col(\"Id\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b518519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast 'Id' column to integer\n",
    "tags_test_df = tags_test_df.withColumn(\"Id\", col(\"Id\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86494fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seed at top, lower vocab size (runs faster), lightly increase regParam from 1.0\n",
    "\n",
    "vocab_size = 100000\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='Body', outputCol='words')\n",
    "#hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features')\n",
    "count_vec = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol='features', vocabSize=vocab_size)\n",
    "logreg = LogisticRegression(maxIter=22, regParam=1.05).setLabelCol('label')\n",
    "\n",
    "tokens = tokenizer.transform(train_tags_df_with_label)\n",
    "#hashes = hashingTF.transform(tokens)\n",
    "countmodel = count_vec.fit(tokens)\n",
    "counts = countmodel.transform(tokens)\n",
    "model = logreg.fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0321bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary = model.summary\n",
    "\n",
    "#print(f'Accuracy on training set: {summary.accuracy}')\n",
    "#print(f'Precision on training set (by label): {summary.precisionByLabel}')\n",
    "#print(f'Recall on training set (by label): {summary.recallByLabel}')\n",
    "#print(f'Areas under the ROC: {summary.areaUnderROC}')\n",
    "\n",
    "#Are you over or under fitting?\n",
    "\n",
    "#Can be determined by looking at training vs test data, if the model is doing better on \n",
    "#the traininig data(or if you have a lot of training data), you may be overfitting, this is where \n",
    "#hyper parameter tuning comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38bb2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents\n",
    "test_tokens = tokenizer.transform(tags_test_df)\n",
    "test_counts = countmodel.transform(test_tokens)\n",
    "\n",
    "predictions = model.transform(test_counts)\n",
    "selected = predictions.select('Id','Body', 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8d2b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the DataFrame by 'Id' column\n",
    "ordered_df = selected.orderBy(\"Id\")\n",
    "\n",
    "# Select the 'prediction' column and collect the values as a list\n",
    "prediction_list = ordered_df.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Convert the collected values to integers\n",
    "prediction_int_list = [int(prediction) for prediction in prediction_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94fa43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p tags i tags and end line characters\n",
    "\n",
    "#body = re.sub(r'<p>|</p>|<i>|</i>|/n',\",body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4823d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what are the top 10 tags, then build prediction\n",
    "#some tags have dashes\n",
    "#some have numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "743f8070",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.9321\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "classification = [0] * 4649\n",
    "\n",
    "grader.score('spark_ml__classification', prediction_int_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e73ec",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## K-means (ungraded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75688c",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "From your trained Word2Vec model, pass the vectors into a K-means clustering algorithm. Create a plot of the sum of squared error by calculating the square root of the sum of the squared distances for each point and its assigned cluster. For an independent variable use either the number of clusters k or the dimension of the Word2Vec vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6fd99",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2023 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
